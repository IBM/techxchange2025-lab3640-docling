{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccb6e4d",
   "metadata": {},
   "source": [
    "# Agentic AI with Docling as MCP tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43266224",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this lab, we will introduce **Docling MCP**, a service that provides tools for document conversion, processing, and generation. It also has extensions for leveraging popular agentic frameworks.\n",
    "\n",
    "It uses the Docling library to convert PDF documents into structured formats and provides a caching mechanism to improve performance. The service exposes functionality through a set of tools that can be called by client applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c4bb8",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd21e20",
   "metadata": {},
   "source": [
    "### Technologies We'll Use\n",
    "\n",
    "Building on our previous labs, we will leverage:\n",
    "\n",
    "1. **[Docling](https://docling-project.github.io/docling/):** An open-source toolkit used to parse and convert documents.\n",
    "2. **[MCP](https://modelcontextprotocol.io)**: The model context protocol for creating a tool.\n",
    "3. **[Llama Stack](https://llama-stack.readthedocs.io/)**: Backend for building generative AI applications exposing standard APIs.\n",
    "4. **[OpenAI Agents SDK](https://openai.github.io/openai-agents-python/)**: A lightweight framework for building agentic AI apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12622cca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0db4ad",
   "metadata": {},
   "source": [
    "#### Notebook dependencies\n",
    "\n",
    "Create a virtual environment to run this notebook, for instance, with [uv](https://docs.astral.sh/uv/), and install the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6405af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: /Users/dol/projects/tx25/techxchange2025-lab3640-docling/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install llama-stack-client openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd38b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "from agents import Agent, ModelSettings, Runner, SQLiteSession, set_trace_processors, set_tracing_disabled, ItemHelpers, FileSearchTool\n",
    "from agents.mcp import MCPServerStreamableHttp\n",
    "from agents.models.openai_provider import OpenAIProvider\n",
    "from agents.run import RunConfig\n",
    "from agents.items import ResponseFunctionToolCall, ResponseFileSearchToolCall\n",
    "from agents.tracing.processors import BatchTraceProcessor, ConsoleSpanExporter\n",
    "from openai import AsyncOpenAI\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "console = Console(width=100, soft_wrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1915319",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLS_URL = \"http://localhost:8321\"\n",
    "BASE_URL = f\"{LLS_URL}/v1/openai/v1\"\n",
    "API_KEY = \"none\"\n",
    "MODEL_ID = \"vllm/gpt-oss-120b\"\n",
    "# MODEL_ID = \"vllm/llama-3-3-70b\"\n",
    "# MODEL_ID = \"vllm/Llama-4-Maverick\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e714354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama Stack Client\n",
    "lls_client = LlamaStackClient(base_url=LLS_URL)\n",
    "\n",
    "# Model client\n",
    "client = AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "# Configure the OpenAI provider that uses our AsyncOpenAI client for Llama Stack\n",
    "provider = OpenAIProvider(openai_client=client)\n",
    "\n",
    "# Tell OpenAI to dump traces to the console\n",
    "set_tracing_disabled(True)\n",
    "set_trace_processors([BatchTraceProcessor(exporter=ConsoleSpanExporter())])\n",
    "\n",
    "# Setup quite logging\n",
    "logging.getLogger(\"openai.agents\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58094489",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions=\"\"\"You are an assistant that uses external tools.  \n",
    "Follow these rules strictly:\n",
    "\n",
    "1. At each reasoning step, you may call **at most one tool**.  \n",
    "2. If you need to use multiple tools, do it sequentially:  \n",
    "   - Call exactly one tool.  \n",
    "   - Wait for its result.  \n",
    "   - Then think again and, if needed, call another tool.  \n",
    "3. Never call two or more tools in the same response.  \n",
    "4. If no tool is needed, just provide your reasoning or final answer.  \n",
    "5. Treat a missing tool result as blocking â€” do not continue reasoning until you receive the output.\n",
    "\n",
    "If you ever attempt to call more than one tool at once, your answer will be rejected.  \n",
    "Always restrict yourself to **zero or one tool call per response**.\n",
    "\n",
    "When the tools return a base64 image, simply create some markdown wrapper which allows to display it and take special care in not hallucinating any byte.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d7be7e",
   "metadata": {},
   "source": [
    "### Agent definition\n",
    "\n",
    "In the following block we define the Document Agent with:\n",
    "\n",
    "1. Connection to the Docling MCP server\n",
    "2. Selection of the tools to use in Docling\n",
    "3. Possibly, use other tools, e.g. the built-in File Search\n",
    "\n",
    "The `run_agent()` method will be called in the use cases below with different user prompts and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4cbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent(queries=[], extra_tools=[]):\n",
    "    async with MCPServerStreamableHttp(\n",
    "        name=\"Docling MCP\",\n",
    "        params={\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"timeout\": 60.0,\n",
    "        },\n",
    "        client_session_timeout_seconds=60,\n",
    "    ) as server:\n",
    "        agent = Agent(\n",
    "            name=\"Document Agent\",\n",
    "            model=MODEL_ID,\n",
    "            instructions=instructions,\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=0, top_p=0.9,\n",
    "                parallel_tool_calls=False,\n",
    "                tool_choice=\"required\",\n",
    "            ),\n",
    "            mcp_servers=[server],\n",
    "            tools=extra_tools,\n",
    "        )\n",
    "        session = SQLiteSession(str(uuid.uuid4()))\n",
    "        print(f\"Created session_id={session.session_id} for Agent({agent.name})\")\n",
    "\n",
    "        user_queries = [instructions, *queries]\n",
    "\n",
    "        for prompt in user_queries:\n",
    "            console.print(f\"[cyan]ðŸ‘¤ User> {prompt}[/cyan]\")\n",
    "            # Launch the agent runner\n",
    "            result = Runner.run_streamed(\n",
    "                agent,\n",
    "                prompt,\n",
    "                session=session,\n",
    "                run_config=RunConfig(model_provider=provider),\n",
    "            )\n",
    "\n",
    "            # Print the events as they appear from the agent stream\n",
    "            async for event in result.stream_events():\n",
    "                # We'll ignore the raw responses event deltas\n",
    "                if event.type == \"raw_response_event\":\n",
    "                    continue\n",
    "                # When the agent updates, print that\n",
    "                elif event.type == \"agent_updated_stream_event\":\n",
    "                    console.print(f\"Agent updated: {event.new_agent.name}\")\n",
    "                    continue\n",
    "                # When items are generated, print them\n",
    "                elif event.type == \"run_item_stream_event\":\n",
    "                    if event.item.type == \"tool_call_item\":\n",
    "                        raw_item = event.item.raw_item\n",
    "                        if isinstance(raw_item, ResponseFunctionToolCall):\n",
    "                            console.print(f\"[yellow]-- Tool was called: {raw_item.name}({raw_item.arguments.strip()})[/yellow]\")\n",
    "                    elif event.item.type == \"tool_call_output_item\":\n",
    "                        console.print(f\"[yellow]-- Tool output: {event.item.output}[/yellow]\")\n",
    "                    elif event.item.type == \"message_output_item\":\n",
    "                        md = Markdown(ItemHelpers.text_message_output(event.item))\n",
    "                        console.print(\"[green]ðŸ¤– Assistant>[/green]\")\n",
    "                        console.print(md)\n",
    "                    else:\n",
    "                        print(f\"other event: {event.item.type}\")\n",
    "                        pass  # Ignore other event types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5513e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04d63c",
   "metadata": {},
   "source": [
    "## 1. Interact with documents\n",
    "\n",
    "In this part of the lab, you will use the agent you just built to work with existing documents. The agent leverages both the reasoning capabilities of the LLM and the Docling MCP tools to interpret, process, and retrieve document content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f577fda",
   "metadata": {},
   "source": [
    "### Document conversion and exports\n",
    "\n",
    "The Docling MCP tool allows to convert documents and return their content to the agent model for reasoning about it.\n",
    "One of the classic use cases is document summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ec6610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcp.client.streamable_http:Received session ID: 768960fd755c421480d49ebe53533144\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-06-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created session_id=5a0a9325-65fd-4459-b3c9-31cd9e0127c9 for Agent(Document Agent)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ðŸ‘¤ User&gt; You are an assistant that uses external tools.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Follow these rules strictly:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\">. At each reasoning step, you may call **at most one tool**.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">. If you need to use multiple tools, do it sequentially:  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Call exactly one tool.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Wait for its result.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Then think again and, if needed, call another tool.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\">. Never call two or more tools in the same response.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\">. If no tool is needed, just provide your reasoning or final answer.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\">. Treat a missing tool result as blocking â€” do not continue reasoning until you receive the output.</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">If you ever attempt to call more than one tool at once, your answer will be rejected.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Always restrict yourself to **zero or one tool call per response**.</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">When the tools return a base64 image, simply create some markdown wrapper which allows to display it and take special care in not hallucinating any byte.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mðŸ‘¤ User> You are an assistant that uses external tools.  \u001b[0m\n",
       "\u001b[36mFollow these rules strictly:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[36m. At each reasoning step, you may call **at most one tool**.  \u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[36m. If you need to use multiple tools, do it sequentially:  \u001b[0m\n",
       "\u001b[36m   - Call exactly one tool.  \u001b[0m\n",
       "\u001b[36m   - Wait for its result.  \u001b[0m\n",
       "\u001b[36m   - Then think again and, if needed, call another tool.  \u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[36m. Never call two or more tools in the same response.  \u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[36m. If no tool is needed, just provide your reasoning or final answer.  \u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[36m. Treat a missing tool result as blocking â€” do not continue reasoning until you receive the output.\u001b[0m\n",
       "\n",
       "\u001b[36mIf you ever attempt to call more than one tool at once, your answer will be rejected.  \u001b[0m\n",
       "\u001b[36mAlways restrict yourself to **zero or one tool call per response**.\u001b[0m\n",
       "\n",
       "\u001b[36mWhen the tools return a base64 image, simply create some markdown wrapper which allows to display it and take special care in not hallucinating any byte.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Agent updated: Document Agent\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Agent updated: Document Agent\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸ¤– Assistant&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸ¤– Assistant>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Understood. I will adhere to the singleâ€‘toolâ€‘perâ€‘step rule and only invoke a tool when necessary, waiting for its result before proceeding. If no tool is required, Iâ€™ll provide the answer directly.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Understood. I will adhere to the singleâ€‘toolâ€‘perâ€‘step rule and only invoke a tool when necessary, waiting for its result before proceeding. If no tool is required, Iâ€™ll provide the answer directly.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ðŸ‘¤ User&gt; Convert the document on </span><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://arxiv.org/pdf/2408.09869</span><span style=\"color: #008080; text-decoration-color: #008080\"> and give me a summary of the document from its markdown content.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mðŸ‘¤ User> Convert the document on \u001b[0m\u001b[4;36mhttps://arxiv.org/pdf/2408.09869\u001b[0m\u001b[36m and give me a summary of the document from its markdown content.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Agent updated: Document Agent\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Agent updated: Document Agent\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">convert_document_into_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"source\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"https://arxiv.org/pdf/2408.09869\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mconvert_document_into_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"source\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"https://arxiv.org/pdf/2408.09869\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"from_cache\\\": false,\\n  \\\"document_key\\\": \\\"a66148ddcb5e4522d218e89ea3276db2\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"from_cache\\\": false,\\n  \\\"document_key\\\": \\\"a66148ddcb5e4522d218e89ea3276db2\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">export_docling_document_to_markdown({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"a66148ddcb5e4522d218e89ea3276db2\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"max_size\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: null</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mexport_docling_document_to_markdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"a66148ddcb5e4522d218e89ea3276db2\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"max_size\"\u001b[0m\u001b[33m: null\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">is_document_in_local_cache({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"a66148ddcb5e4522d218e89ea3276db2\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mis_document_in_local_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"a66148ddcb5e4522d218e89ea3276db2\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">convert_document_into_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"source\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"https://arxiv.org/pdf/2408.09869\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mconvert_document_into_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"source\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"https://arxiv.org/pdf/2408.09869\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">export_docling_document_to_markdown({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"a66148ddcb5e4522d218e89ea3276db2\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"max_size\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: null</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mexport_docling_document_to_markdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"a66148ddcb5e4522d218e89ea3276db2\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"max_size\"\u001b[0m\u001b[33m: null\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">convert_document_into_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"source\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"https://arxiv.org/pdf/2408.09869.pdf\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mconvert_document_into_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"source\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"https://arxiv.org/pdf/2408.09869.pdf\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"a66148ddcb5e4522d218e89ea3276db2\\\",\\n  \\\"markdown\\\": \\\"&lt;!-- image --&gt;\\\\n\\\\n## Docling Technical Report\\\\n\\\\n## Version 1.0\\\\n\\\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\\\n\\\\nAI4K Group, IBM Research RÂ¨ uschlikon, Switzerland\\\\n\\\\n## Abstract\\\\n\\\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\\\\n\\\\n## 1 Introduction\\\\n\\\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\\\n\\\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\\\n\\\\nHere is what Docling delivers today:\\\\n\\\\n- Converts PDF documents to JSON or Markdown format, stable and lightning fast\\\\n- Understands detailed page layout, reading order, locates figures and recovers table structures\\\\n- Extracts metadata from the document, such as title, authors, references and language\\\\n- Optionally applies OCR, e.g. for scanned PDFs\\\\n- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\\\n- Can leverage different accelerators (GPU, MPS, etc).\\\\n\\\\n## 2 Getting Started\\\\n\\\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\\\n\\\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\\\n\\\\nfrom docling.document\\\\\\\\_converter import DocumentConverter\\\\n\\\\n```\\\\nsource = \\\\\\\"https://arxiv.org/pdf/2206.01062\\\\\\\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \\\\\\\"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\\\\\\\"\\\\n```\\\\n\\\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\\\\n\\\\n## 3 Processing pipeline\\\\n\\\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\\\\n\\\\n## 3.1 PDF backends\\\\n\\\\nTwo basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\\\n\\\\n1 see huggingface.co/ds4sd/docling-models/\\\\n\\\\nFigure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nlicensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\\\\n\\\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\\\\n\\\\n## 3.2 AI models\\\\n\\\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\\\\n\\\\n## Layout Analysis Model\\\\n\\\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\\\\n\\\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\\\\n\\\\n## Table Structure Recognition\\\\n\\\\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\\\\n\\\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\\\\n\\\\n## OCR\\\\n\\\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\\\\n\\\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\\\\n\\\\n## 3.3 Assembly\\\\n\\\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\\\\n\\\\n## 3.4 Extensibility\\\\n\\\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\\\n\\\\nImplementations of model classes must satisfy the python Callable interface. The \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\\\\n\\\\n## 4 Performance\\\\n\\\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\\\\n\\\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\\\\n\\\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\\\n\\\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\\\n\\\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\\\\n\\\\n| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\\\\n|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\\\\n|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\\\\n| Apple M3 Max (16 cores) | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\\\\n| Intel(R) Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\\\\n\\\\n## 5 Applications\\\\n\\\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\\\\n\\\\n## 6 Future work and contributions\\\\n\\\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\\\n\\\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\\\\n\\\\n## References\\\\n\\\\n- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\\\\n- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\\\\n\\\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\\\\n\\\\n- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\\\\n- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\\\n- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\\\n- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\\\n- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\\\\n- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\\\\\\\_index .\\\\n- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San JosÂ´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\\\\\\\_3 .\\\\n- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\\\\n- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\\\n- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\\\n- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\\\\n- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\\\n- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\\\n- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\\\\n\\\\n## Appendix\\\\n\\\\nIn this section, we illustrate a few examples of Docling's output in Markdown and JSON.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\n1 INTRODUCTION\\\\n\\\\nDespite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\\\n\\\\n|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\\\\n|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\\\\n| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\\\\n\\\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\\\n\\\\n## 5 EXPERIMENTS\\\\n\\\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\\\n\\\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\\\n\\\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\\\n\\\\nmak enbrel\\\\n\\\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\\\\n\\\\nThird, achienec\\\\n\\\\n## EXPERIMENTS\\\\n\\\\nchalenongayouls ground-vuth dawa such WC\\\\n\\\\ncoioct dcochon modols\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\\\n\\\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nof row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\\\\n\\\\nonly. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\\\\n\\\\nwere assembled and supervised.\\\\n\\\\nwhile coverage ensures that all meaningful items on a page can to a document category, such as\\\\n\\\\nbe annotated. We refrained from class labels that are very specific\\\\n\\\\nAbstract in the\\\\n\\\\nScientific Articles semantics of the text. Labels such as\\\\n\\\\ncategory. We also avoided class labels that are tightly linked to the\\\\n\\\\nAuthor\\\\n\\\\nAffiliation\\\\n\\\\nteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\\\\n\\\\nand\\\\n\\\\n, as seen\\\\n\\\\nPhase 1: Data selection and preparation.\\\\n\\\\nOur inclusion cri-\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"a66148ddcb5e4522d218e89ea3276db2\\\",\\n  \\\"markdown\\\": \\\"\u001b[0m\u001b[33m<\u001b[0m\u001b[33m!-- image -->\\\\n\\\\n## Docling Technical Report\\\\n\\\\n## Version 1.0\\\\n\\\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\\\n\\\\nAI4K Group, IBM Research RÂ¨ uschlikon, Switzerland\\\\n\\\\n## Abstract\\\\n\\\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDocLayNet\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and table structure recognition \u001b[0m\u001b[33m(\u001b[0m\u001b[33mTableFormer\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\\\\n\\\\n## 1 Introduction\\\\n\\\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mRAG\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings \u001b[0m\u001b[33m[\u001b[0m\u001b[33m3\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\\\n\\\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12, 13, 9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\\\n\\\\nHere is what Docling delivers today:\\\\n\\\\n- Converts PDF documents to JSON or Markdown format, stable and lightning fast\\\\n- Understands detailed page layout, reading order, locates figures and recovers table structures\\\\n- Extracts metadata from the document, such as title, authors, references and language\\\\n- Optionally applies OCR, e.g. for scanned PDFs\\\\n- Can be configured to be optimal for batch-mode \u001b[0m\u001b[33m(\u001b[0m\u001b[33mi.e high throughput, low time-to-solution\u001b[0m\u001b[33m)\u001b[0m\u001b[33m or interactive mode \u001b[0m\u001b[33m(\u001b[0m\u001b[33mcompromise on efficiency, low time-to-solution\u001b[0m\u001b[33m)\u001b[0m\u001b[33m\\\\n- Can leverage different accelerators \u001b[0m\u001b[33m(\u001b[0m\u001b[33mGPU, MPS, etc\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\n## 2 Getting Started\\\\n\\\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\\\n\\\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\\\n\\\\nfrom docling.document\\\\\\\\_converter import DocumentConverter\\\\n\\\\n```\\\\nsource = \\\\\\\"https://arxiv.org/pdf/2206.01062\\\\\\\" # PDF path or URL converter = DocumentConverter\u001b[0m\u001b[33m(\u001b[0m\u001b[33m)\u001b[0m\u001b[33m result = converter.convert_single\u001b[0m\u001b[33m(\u001b[0m\u001b[33msource\u001b[0m\u001b[33m)\u001b[0m\u001b[33m print\u001b[0m\u001b[33m(\u001b[0m\u001b[33mresult.render_as_markdown\u001b[0m\u001b[33m(\u001b[0m\u001b[33m)\u001b[0m\u001b[33m)\u001b[0m\u001b[33m # output: \\\\\\\"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis \u001b[0m\u001b[33m[\u001b[0m\u001b[33m...\u001b[0m\u001b[33m]\u001b[0m\u001b[33m\\\\\\\"\\\\n```\\\\n\\\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features \u001b[0m\u001b[33m(\u001b[0m\u001b[33me.g. OCR, table structure recognition\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\\\\n\\\\n## 3 Processing pipeline\\\\n\\\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document \u001b[0m\u001b[33m(\u001b[0m\u001b[33msee Fig. 1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\\\\n\\\\n## 3.1 PDF backends\\\\n\\\\nTwo basic requirements to process PDF documents in our pipeline are a\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to retrieve all text content and their geometric coordinates on each page and b\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\\\n\\\\n1 see huggingface.co/ds4sd/docling-models/\\\\n\\\\nFigure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\\\n\\\\n<!-- image -->\\\\n\\\\nlicensing \u001b[0m\u001b[33m(\u001b[0m\u001b[33me.g. pymupdf \u001b[0m\u001b[33m[\u001b[0m\u001b[33m7\u001b[0m\u001b[33m]\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns \u001b[0m\u001b[33m(\u001b[0m\u001b[33mpypdfium, PyPDF\u001b[0m\u001b[33m)\u001b[0m\u001b[33m \u001b[0m\u001b[33m[\u001b[0m\u001b[33m15, 14\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf \u001b[0m\u001b[33m[\u001b[0m\u001b[33m4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\\\\n\\\\n## 3.2 AI models\\\\n\\\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. The second model is TableFormer \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12, 9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, a state-of-the-art table structure recognition model. We provide the pre-trained weights \u001b[0m\u001b[33m(\u001b[0m\u001b[33mhosted on huggingface\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\\\\n\\\\n## Layout Analysis Model\\\\n\\\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and re-trained on DocLayNet \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\\\\n\\\\n## Table Structure Recognition\\\\n\\\\nThe TableFormer model \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, first published in 2022 and since refined with a custom structure token language \u001b[0m\u001b[33m[\u001b[0m\u001b[33m9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch \u001b[0m\u001b[33m[\u001b[0m\u001b[33m2\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\\\\n\\\\n## OCR\\\\n\\\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image \u001b[0m\u001b[33m(\u001b[0m\u001b[33m216 dpi\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU \u001b[0m\u001b[33m(\u001b[0m\u001b[33mupwards of 30 seconds per page\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\\\\n\\\\n## 3.3 Assembly\\\\n\\\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\\\\n\\\\n## 3.4 Extensibility\\\\n\\\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass \u001b[0m\u001b[33m(\u001b[0m\u001b[33m BaseModelPipeline \u001b[0m\u001b[33m)\u001b[0m\u001b[33m or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\\\n\\\\nImplementations of model classes must satisfy the python Callable interface. The \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\\\\n\\\\n## 4 Performance\\\\n\\\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget \u001b[0m\u001b[33m(\u001b[0m\u001b[33mthrough setting OMP NUM THREADS environment variable \u001b[0m\u001b[33m)\u001b[0m\u001b[33m once to 4 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDocling default\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and once to 16 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mequal to full core count on the test hardware\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. All results are shown in Table 1.\\\\n\\\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\\\\n\\\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\\\n\\\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\\\n\\\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution \u001b[0m\u001b[33m(\u001b[0m\u001b[33mTTS\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, computed throughput in pages per second, and the peak memory used \u001b[0m\u001b[33m(\u001b[0m\u001b[33mresident set size\u001b[0m\u001b[33m)\u001b[0m\u001b[33m for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\\\\n\\\\n| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\\\\n|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\\\\n|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\\\\n| Apple M3 Max \u001b[0m\u001b[33m(\u001b[0m\u001b[33m16 cores\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\\\\n| Intel\u001b[0m\u001b[33m(\u001b[0m\u001b[33mR\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\\\\n\\\\n## 5 Applications\\\\n\\\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mRAG\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex \u001b[0m\u001b[33m[\u001b[0m\u001b[33m8\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11, 10\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Docling is also integrated within the open IBM data prep kit \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, which implements scalable data transforms to build large-scale multi-modal training datasets.\\\\n\\\\n## 6 Future work and contributions\\\\n\\\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\\\n\\\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\\\\n\\\\n## References\\\\n\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1\u001b[0m\u001b[33m]\u001b[0m\u001b[33m J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m2\u001b[0m\u001b[33m]\u001b[0m\u001b[33m J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\\\\n\\\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mASPLOS '24\u001b[0m\u001b[33m)\u001b[0m\u001b[33m . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\\\\n\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m3\u001b[0m\u001b[33m]\u001b[0m\u001b[33m C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing \u001b[0m\u001b[33m(\u001b[0m\u001b[33mCLOUD\u001b[0m\u001b[33m)\u001b[0m\u001b[33m , pages 363-373. IEEE, 2022.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m7\u001b[0m\u001b[33m]\u001b[0m\u001b[33m A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m8\u001b[0m\u001b[33m]\u001b[0m\u001b[33m J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\\\\\\\_index .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San JosÂ´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\\\\\\\_3 .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m10\u001b[0m\u001b[33m]\u001b[0m\u001b[33m L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change \u001b[0m\u001b[33m(\u001b[0m\u001b[33mClimateNLP 2024\u001b[0m\u001b[33m)\u001b[0m\u001b[33m , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11\u001b[0m\u001b[33m]\u001b[0m\u001b[33m L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15\u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m:6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m14\u001b[0m\u001b[33m]\u001b[0m\u001b[33m pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m15\u001b[0m\u001b[33m]\u001b[0m\u001b[33m P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\\\\n\\\\n## Appendix\\\\n\\\\nIn this section, we illustrate a few examples of Docling's output in Markdown and JSON.\\\\n\\\\n<!-- image -->\\\\n\\\\n1 INTRODUCTION\\\\n\\\\nDespite the substantial improvements achieved with machine-learning \u001b[0m\u001b[33m(\u001b[0m\u001b[33mML\u001b[0m\u001b[33m)\u001b[0m\u001b[33m approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1-4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. The challenge originates from the huge variability in PDF documents regarding layout, language and formats \u001b[0m\u001b[33m(\u001b[0m\u001b[33mscanned, programmatic or a combination of both\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper \u001b[0m\u001b[33m(\u001b[0m\u001b[33marxiv.org/pdf/2206.01062\u001b[0m\u001b[33m)\u001b[0m\u001b[33m - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mnot shown\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 2: Prediction performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of object detection networks on DocLayNet test set. The MRCNN \u001b[0m\u001b[33m(\u001b[0m\u001b[33mMask R-CNN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and FRCNN \u001b[0m\u001b[33m(\u001b[0m\u001b[33mFaster R-CNN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo \u001b[0m\u001b[33m(\u001b[0m\u001b[33mMask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, with default configurations. The YOLO implementation utilized was YOLOv5x6 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\\\n\\\\n|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\\\\n|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\\\\n| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\\\\n\\\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\\\n\\\\n## 5 EXPERIMENTS\\\\n\\\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and the availability of general frameworks such as detectron2 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m17\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\\\n\\\\nFigure 5: Prediction performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\\\n\\\\n<!-- image -->\\\\n\\\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\\\n\\\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. These scores are computed by leveraging the evaluation code provided by the COCO API \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nIn Table 2, we present baseline experiments \u001b[0m\u001b[33m(\u001b[0m\u001b[33mgiven in mAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m on Mask R-CNN \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, Faster R-CNN \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, and YOLOv5 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\\\n\\\\nmak enbrel\\\\n\\\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\\\\n\\\\nThird, achienec\\\\n\\\\n## EXPERIMENTS\\\\n\\\\nchalenongayouls ground-vuth dawa such WC\\\\n\\\\ncoioct dcochon modols\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\\\n\\\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence \u001b[0m\u001b[33m(\u001b[0m\u001b[33mas %\\\\n\\\\n<!-- image --\u001b[0m\u001b[33m>\u001b[0m\u001b[33m\\\\n\\\\nof row 'Total'\u001b[0m\u001b[33m)\u001b[0m\u001b[33m in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\\\\n\\\\nonly. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\\\\n\\\\nwere assembled and supervised.\\\\n\\\\nwhile coverage ensures that all meaningful items on a page can to a document category, such as\\\\n\\\\nbe annotated. We refrained from class labels that are very specific\\\\n\\\\nAbstract in the\\\\n\\\\nScientific Articles semantics of the text. Labels such as\\\\n\\\\ncategory. We also avoided class labels that are tightly linked to the\\\\n\\\\nAuthor\\\\n\\\\nAffiliation\\\\n\\\\nteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF \u001b[0m\u001b[33m(\u001b[0m\u001b[33mA\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, as rendered Markdown \u001b[0m\u001b[33m(\u001b[0m\u001b[33mB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and in JSON representation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mC\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m', is repeated for each column in the Markdown representation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell \u001b[0m\u001b[33m(\u001b[0m\u001b[33mC\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nand\\\\n\\\\n, as seen\\\\n\\\\nPhase 1: Data selection and preparation.\\\\n\\\\nOur inclusion cri-\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"in_cache\\\": true\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"in_cache\\\": true\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"from_cache\\\": true,\\n  \\\"document_key\\\": \\\"a66148ddcb5e4522d218e89ea3276db2\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"from_cache\\\": true,\\n  \\\"document_key\\\": \\\"a66148ddcb5e4522d218e89ea3276db2\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"a66148ddcb5e4522d218e89ea3276db2\\\",\\n  \\\"markdown\\\": \\\"&lt;!-- image --&gt;\\\\n\\\\n## Docling Technical Report\\\\n\\\\n## Version 1.0\\\\n\\\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\\\n\\\\nAI4K Group, IBM Research RÂ¨ uschlikon, Switzerland\\\\n\\\\n## Abstract\\\\n\\\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\\\\n\\\\n## 1 Introduction\\\\n\\\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\\\n\\\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\\\n\\\\nHere is what Docling delivers today:\\\\n\\\\n- Converts PDF documents to JSON or Markdown format, stable and lightning fast\\\\n- Understands detailed page layout, reading order, locates figures and recovers table structures\\\\n- Extracts metadata from the document, such as title, authors, references and language\\\\n- Optionally applies OCR, e.g. for scanned PDFs\\\\n- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\\\n- Can leverage different accelerators (GPU, MPS, etc).\\\\n\\\\n## 2 Getting Started\\\\n\\\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\\\n\\\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\\\n\\\\nfrom docling.document\\\\\\\\_converter import DocumentConverter\\\\n\\\\n```\\\\nsource = \\\\\\\"https://arxiv.org/pdf/2206.01062\\\\\\\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \\\\\\\"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\\\\\\\"\\\\n```\\\\n\\\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\\\\n\\\\n## 3 Processing pipeline\\\\n\\\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\\\\n\\\\n## 3.1 PDF backends\\\\n\\\\nTwo basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\\\n\\\\n1 see huggingface.co/ds4sd/docling-models/\\\\n\\\\nFigure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nlicensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\\\\n\\\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\\\\n\\\\n## 3.2 AI models\\\\n\\\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\\\\n\\\\n## Layout Analysis Model\\\\n\\\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\\\\n\\\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\\\\n\\\\n## Table Structure Recognition\\\\n\\\\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\\\\n\\\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\\\\n\\\\n## OCR\\\\n\\\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\\\\n\\\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\\\\n\\\\n## 3.3 Assembly\\\\n\\\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\\\\n\\\\n## 3.4 Extensibility\\\\n\\\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\\\n\\\\nImplementations of model classes must satisfy the python Callable interface. The \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\\\\n\\\\n## 4 Performance\\\\n\\\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\\\\n\\\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\\\\n\\\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\\\n\\\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\\\n\\\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\\\\n\\\\n| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\\\\n|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\\\\n|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\\\\n| Apple M3 Max (16 cores) | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\\\\n| Intel(R) Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\\\\n\\\\n## 5 Applications\\\\n\\\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\\\\n\\\\n## 6 Future work and contributions\\\\n\\\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\\\n\\\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\\\\n\\\\n## References\\\\n\\\\n- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\\\\n- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\\\\n\\\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\\\\n\\\\n- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\\\\n- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\\\n- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\\\n- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\\\n- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\\\\n- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\\\\\\\_index .\\\\n- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San JosÂ´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\\\\\\\_3 .\\\\n- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\\\\n- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\\\n- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\\\n- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\\\\n- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\\\n- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\\\n- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\\\\n\\\\n## Appendix\\\\n\\\\nIn this section, we illustrate a few examples of Docling's output in Markdown and JSON.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\n1 INTRODUCTION\\\\n\\\\nDespite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\\\n\\\\n|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\\\\n|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\\\\n| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\\\\n\\\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\\\n\\\\n## 5 EXPERIMENTS\\\\n\\\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\\\n\\\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\\\n\\\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\\\n\\\\nmak enbrel\\\\n\\\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\\\\n\\\\nThird, achienec\\\\n\\\\n## EXPERIMENTS\\\\n\\\\nchalenongayouls ground-vuth dawa such WC\\\\n\\\\ncoioct dcochon modols\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\\\n\\\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nof row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\\\\n\\\\nonly. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\\\\n\\\\nwere assembled and supervised.\\\\n\\\\nwhile coverage ensures that all meaningful items on a page can to a document category, such as\\\\n\\\\nbe annotated. We refrained from class labels that are very specific\\\\n\\\\nAbstract in the\\\\n\\\\nScientific Articles semantics of the text. Labels such as\\\\n\\\\ncategory. We also avoided class labels that are tightly linked to the\\\\n\\\\nAuthor\\\\n\\\\nAffiliation\\\\n\\\\nteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\\\\n\\\\nand\\\\n\\\\n, as seen\\\\n\\\\nPhase 1: Data selection and preparation.\\\\n\\\\nOur inclusion cri-\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"a66148ddcb5e4522d218e89ea3276db2\\\",\\n  \\\"markdown\\\": \\\"\u001b[0m\u001b[33m<\u001b[0m\u001b[33m!-- image -->\\\\n\\\\n## Docling Technical Report\\\\n\\\\n## Version 1.0\\\\n\\\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\\\n\\\\nAI4K Group, IBM Research RÂ¨ uschlikon, Switzerland\\\\n\\\\n## Abstract\\\\n\\\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDocLayNet\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and table structure recognition \u001b[0m\u001b[33m(\u001b[0m\u001b[33mTableFormer\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\\\\n\\\\n## 1 Introduction\\\\n\\\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mRAG\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings \u001b[0m\u001b[33m[\u001b[0m\u001b[33m3\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\\\n\\\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12, 13, 9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\\\n\\\\nHere is what Docling delivers today:\\\\n\\\\n- Converts PDF documents to JSON or Markdown format, stable and lightning fast\\\\n- Understands detailed page layout, reading order, locates figures and recovers table structures\\\\n- Extracts metadata from the document, such as title, authors, references and language\\\\n- Optionally applies OCR, e.g. for scanned PDFs\\\\n- Can be configured to be optimal for batch-mode \u001b[0m\u001b[33m(\u001b[0m\u001b[33mi.e high throughput, low time-to-solution\u001b[0m\u001b[33m)\u001b[0m\u001b[33m or interactive mode \u001b[0m\u001b[33m(\u001b[0m\u001b[33mcompromise on efficiency, low time-to-solution\u001b[0m\u001b[33m)\u001b[0m\u001b[33m\\\\n- Can leverage different accelerators \u001b[0m\u001b[33m(\u001b[0m\u001b[33mGPU, MPS, etc\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\n## 2 Getting Started\\\\n\\\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\\\n\\\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\\\n\\\\nfrom docling.document\\\\\\\\_converter import DocumentConverter\\\\n\\\\n```\\\\nsource = \\\\\\\"https://arxiv.org/pdf/2206.01062\\\\\\\" # PDF path or URL converter = DocumentConverter\u001b[0m\u001b[33m(\u001b[0m\u001b[33m)\u001b[0m\u001b[33m result = converter.convert_single\u001b[0m\u001b[33m(\u001b[0m\u001b[33msource\u001b[0m\u001b[33m)\u001b[0m\u001b[33m print\u001b[0m\u001b[33m(\u001b[0m\u001b[33mresult.render_as_markdown\u001b[0m\u001b[33m(\u001b[0m\u001b[33m)\u001b[0m\u001b[33m)\u001b[0m\u001b[33m # output: \\\\\\\"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis \u001b[0m\u001b[33m[\u001b[0m\u001b[33m...\u001b[0m\u001b[33m]\u001b[0m\u001b[33m\\\\\\\"\\\\n```\\\\n\\\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features \u001b[0m\u001b[33m(\u001b[0m\u001b[33me.g. OCR, table structure recognition\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\\\\n\\\\n## 3 Processing pipeline\\\\n\\\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document \u001b[0m\u001b[33m(\u001b[0m\u001b[33msee Fig. 1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\\\\n\\\\n## 3.1 PDF backends\\\\n\\\\nTwo basic requirements to process PDF documents in our pipeline are a\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to retrieve all text content and their geometric coordinates on each page and b\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\\\n\\\\n1 see huggingface.co/ds4sd/docling-models/\\\\n\\\\nFigure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\\\n\\\\n<!-- image -->\\\\n\\\\nlicensing \u001b[0m\u001b[33m(\u001b[0m\u001b[33me.g. pymupdf \u001b[0m\u001b[33m[\u001b[0m\u001b[33m7\u001b[0m\u001b[33m]\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns \u001b[0m\u001b[33m(\u001b[0m\u001b[33mpypdfium, PyPDF\u001b[0m\u001b[33m)\u001b[0m\u001b[33m \u001b[0m\u001b[33m[\u001b[0m\u001b[33m15, 14\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf \u001b[0m\u001b[33m[\u001b[0m\u001b[33m4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\\\\n\\\\n## 3.2 AI models\\\\n\\\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. The second model is TableFormer \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12, 9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, a state-of-the-art table structure recognition model. We provide the pre-trained weights \u001b[0m\u001b[33m(\u001b[0m\u001b[33mhosted on huggingface\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\\\\n\\\\n## Layout Analysis Model\\\\n\\\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and re-trained on DocLayNet \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\\\\n\\\\n## Table Structure Recognition\\\\n\\\\nThe TableFormer model \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, first published in 2022 and since refined with a custom structure token language \u001b[0m\u001b[33m[\u001b[0m\u001b[33m9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch \u001b[0m\u001b[33m[\u001b[0m\u001b[33m2\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\\\\n\\\\n## OCR\\\\n\\\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image \u001b[0m\u001b[33m(\u001b[0m\u001b[33m216 dpi\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU \u001b[0m\u001b[33m(\u001b[0m\u001b[33mupwards of 30 seconds per page\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\\\\n\\\\n## 3.3 Assembly\\\\n\\\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\\\\n\\\\n## 3.4 Extensibility\\\\n\\\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass \u001b[0m\u001b[33m(\u001b[0m\u001b[33m BaseModelPipeline \u001b[0m\u001b[33m)\u001b[0m\u001b[33m or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\\\n\\\\nImplementations of model classes must satisfy the python Callable interface. The \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\\\\n\\\\n## 4 Performance\\\\n\\\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget \u001b[0m\u001b[33m(\u001b[0m\u001b[33mthrough setting OMP NUM THREADS environment variable \u001b[0m\u001b[33m)\u001b[0m\u001b[33m once to 4 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDocling default\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and once to 16 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mequal to full core count on the test hardware\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. All results are shown in Table 1.\\\\n\\\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\\\\n\\\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\\\n\\\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\\\n\\\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution \u001b[0m\u001b[33m(\u001b[0m\u001b[33mTTS\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, computed throughput in pages per second, and the peak memory used \u001b[0m\u001b[33m(\u001b[0m\u001b[33mresident set size\u001b[0m\u001b[33m)\u001b[0m\u001b[33m for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\\\\n\\\\n| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\\\\n|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\\\\n|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\\\\n| Apple M3 Max \u001b[0m\u001b[33m(\u001b[0m\u001b[33m16 cores\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\\\\n| Intel\u001b[0m\u001b[33m(\u001b[0m\u001b[33mR\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\\\\n\\\\n## 5 Applications\\\\n\\\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mRAG\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex \u001b[0m\u001b[33m[\u001b[0m\u001b[33m8\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11, 10\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Docling is also integrated within the open IBM data prep kit \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, which implements scalable data transforms to build large-scale multi-modal training datasets.\\\\n\\\\n## 6 Future work and contributions\\\\n\\\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\\\n\\\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\\\\n\\\\n## References\\\\n\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1\u001b[0m\u001b[33m]\u001b[0m\u001b[33m J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m2\u001b[0m\u001b[33m]\u001b[0m\u001b[33m J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\\\\n\\\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mASPLOS '24\u001b[0m\u001b[33m)\u001b[0m\u001b[33m . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\\\\n\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m3\u001b[0m\u001b[33m]\u001b[0m\u001b[33m C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing \u001b[0m\u001b[33m(\u001b[0m\u001b[33mCLOUD\u001b[0m\u001b[33m)\u001b[0m\u001b[33m , pages 363-373. IEEE, 2022.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m7\u001b[0m\u001b[33m]\u001b[0m\u001b[33m A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m8\u001b[0m\u001b[33m]\u001b[0m\u001b[33m J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\\\\\\\_index .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San JosÂ´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\\\\\\\_3 .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m10\u001b[0m\u001b[33m]\u001b[0m\u001b[33m L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change \u001b[0m\u001b[33m(\u001b[0m\u001b[33mClimateNLP 2024\u001b[0m\u001b[33m)\u001b[0m\u001b[33m , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11\u001b[0m\u001b[33m]\u001b[0m\u001b[33m L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15\u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m:6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m14\u001b[0m\u001b[33m]\u001b[0m\u001b[33m pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m15\u001b[0m\u001b[33m]\u001b[0m\u001b[33m P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\\\\n\\\\n## Appendix\\\\n\\\\nIn this section, we illustrate a few examples of Docling's output in Markdown and JSON.\\\\n\\\\n<!-- image -->\\\\n\\\\n1 INTRODUCTION\\\\n\\\\nDespite the substantial improvements achieved with machine-learning \u001b[0m\u001b[33m(\u001b[0m\u001b[33mML\u001b[0m\u001b[33m)\u001b[0m\u001b[33m approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1-4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. The challenge originates from the huge variability in PDF documents regarding layout, language and formats \u001b[0m\u001b[33m(\u001b[0m\u001b[33mscanned, programmatic or a combination of both\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper \u001b[0m\u001b[33m(\u001b[0m\u001b[33marxiv.org/pdf/2206.01062\u001b[0m\u001b[33m)\u001b[0m\u001b[33m - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mnot shown\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 2: Prediction performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of object detection networks on DocLayNet test set. The MRCNN \u001b[0m\u001b[33m(\u001b[0m\u001b[33mMask R-CNN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and FRCNN \u001b[0m\u001b[33m(\u001b[0m\u001b[33mFaster R-CNN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo \u001b[0m\u001b[33m(\u001b[0m\u001b[33mMask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, with default configurations. The YOLO implementation utilized was YOLOv5x6 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\\\n\\\\n|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\\\\n|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\\\\n| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\\\\n\\\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\\\n\\\\n## 5 EXPERIMENTS\\\\n\\\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and the availability of general frameworks such as detectron2 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m17\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\\\n\\\\nFigure 5: Prediction performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\\\n\\\\n<!-- image -->\\\\n\\\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\\\n\\\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. These scores are computed by leveraging the evaluation code provided by the COCO API \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nIn Table 2, we present baseline experiments \u001b[0m\u001b[33m(\u001b[0m\u001b[33mgiven in mAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m on Mask R-CNN \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, Faster R-CNN \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, and YOLOv5 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\\\n\\\\nmak enbrel\\\\n\\\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\\\\n\\\\nThird, achienec\\\\n\\\\n## EXPERIMENTS\\\\n\\\\nchalenongayouls ground-vuth dawa such WC\\\\n\\\\ncoioct dcochon modols\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\\\n\\\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence \u001b[0m\u001b[33m(\u001b[0m\u001b[33mas %\\\\n\\\\n<!-- image --\u001b[0m\u001b[33m>\u001b[0m\u001b[33m\\\\n\\\\nof row 'Total'\u001b[0m\u001b[33m)\u001b[0m\u001b[33m in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\\\\n\\\\nonly. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\\\\n\\\\nwere assembled and supervised.\\\\n\\\\nwhile coverage ensures that all meaningful items on a page can to a document category, such as\\\\n\\\\nbe annotated. We refrained from class labels that are very specific\\\\n\\\\nAbstract in the\\\\n\\\\nScientific Articles semantics of the text. Labels such as\\\\n\\\\ncategory. We also avoided class labels that are tightly linked to the\\\\n\\\\nAuthor\\\\n\\\\nAffiliation\\\\n\\\\nteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF \u001b[0m\u001b[33m(\u001b[0m\u001b[33mA\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, as rendered Markdown \u001b[0m\u001b[33m(\u001b[0m\u001b[33mB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and in JSON representation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mC\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m', is repeated for each column in the Markdown representation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell \u001b[0m\u001b[33m(\u001b[0m\u001b[33mC\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nand\\\\n\\\\n, as seen\\\\n\\\\nPhase 1: Data selection and preparation.\\\\n\\\\nOur inclusion cri-\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"from_cache\\\": false,\\n  \\\"document_key\\\": \\\"994956e127ac4fc83092023df93360da\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"from_cache\\\": false,\\n  \\\"document_key\\\": \\\"994956e127ac4fc83092023df93360da\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸ¤– Assistant&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸ¤– Assistant>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Summary of â€œDocling Technical Reportâ€ (arXivâ€¯2408.09869)</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">What is Docling?</span>\n",
       "Docling is an openâ€‘source (MITâ€‘licensed) Python library for converting PDF documents into structured, machineâ€‘readable formats (JSON or Markdown). It runs locally on commodity hardware and is designed to be fast, stable, and easy to extend.\n",
       "\n",
       "<span style=\"font-weight: bold\">Key Capabilities</span>\n",
       "\n",
       "                                                                                                    \n",
       " <span style=\"font-weight: bold\"> Feature                     </span> <span style=\"font-weight: bold\"> Description                                                        </span> \n",
       " â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \n",
       "  <span style=\"font-weight: bold\">Layout analysis</span>               Uses a custom objectâ€‘detector (RTâ€‘DETRâ€‘based) trained on the        \n",
       "                                large, humanâ€‘annotated DocLayNet dataset to identify page elements  \n",
       "                                (paragraphs, headings, lists, figures, tables, etc.).               \n",
       "  <span style=\"font-weight: bold\">Table structure recognition</span>   Employs TableFormer, a visionâ€‘transformer model that recovers       \n",
       "                                logical row/column structures, spanning cells, headers, and         \n",
       "                                hierarchies.                                                        \n",
       "  <span style=\"font-weight: bold\">OCR (optional)</span>                Integrates EasyOCR for scanned PDFs (highâ€‘resolution 216â€¯dpi        \n",
       "                                input).                                                             \n",
       "  <span style=\"font-weight: bold\">Metadata extraction</span>           Detects title, authors, references, language, and other             \n",
       "                                bibliographic information.                                          \n",
       "  <span style=\"font-weight: bold\">Configurable pipelines</span>        Users can enable/disable OCR, table recognition, set thread         \n",
       "                                budgets, and choose GPU/MPS acceleration.                           \n",
       "  <span style=\"font-weight: bold\">Extensibility</span>                 The model pipeline is modular; developers can subclass the base     \n",
       "                                pipeline to add new models (e.g., figure classifier, equation       \n",
       "                                recognizer).                                                        \n",
       "  <span style=\"font-weight: bold\">Batch &amp; interactive modes</span>     Optimized for highâ€‘throughput batch processing or lowâ€‘latency       \n",
       "                                interactive use.                                                    \n",
       "                                                                                                    \n",
       "\n",
       "<span style=\"font-weight: bold\">Processing Pipeline</span>\n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">PDF backend</span> â€“ extracts text tokens with coordinates and renders page images. Two backends are off\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">AI model stage</span> â€“ runs layout analysis and tableâ€‘structure models on each page image.             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">Postâ€‘processing</span> â€“ merges predictions, fixes reading order, adds language detection, and assembles\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">Serialization</span> â€“ outputs JSON or Markdown.                                                        \n",
       "\n",
       "<span style=\"font-weight: bold\">Performance (reference hardware)</span>\n",
       "\n",
       "                                                                                                    \n",
       " <span style=\"font-weight: bold\">                     </span> <span style=\"font-weight: bold\">         </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\"> Timeâ€‘toâ€‘solution     </span> <span style=\"font-weight: bold\"> Throughput          </span> <span style=\"font-weight: bold\">          </span> \n",
       " <span style=\"font-weight: bold\"> System              </span> <span style=\"font-weight: bold\"> Threads </span> <span style=\"font-weight: bold\"> Backend  </span> <span style=\"font-weight: bold\"> (225â€¯pages)          </span> <span style=\"font-weight: bold\"> (pages/s)           </span> <span style=\"font-weight: bold\"> Peak RAM </span> \n",
       " â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \n",
       "  Appleâ€¯M3â€¯Max (16â€¯     4         native     177â€¯s                  1.27                  6.2â€¯GB    \n",
       "  cores)                                                                                            \n",
       "  Appleâ€¯M3â€¯Max (16â€¯     16        native     167â€¯s                  1.34                  6.2â€¯GB    \n",
       "  cores)                                                                                            \n",
       "  Appleâ€¯M3â€¯Max (16â€¯     4         pypdfium   103â€¯s                  2.18                  2.6â€¯GB    \n",
       "  cores)                                                                                            \n",
       "  Appleâ€¯M3â€¯Max (16â€¯     16        pypdfium   92â€¯s                   2.45                  2.6â€¯GB    \n",
       "  cores)                                                                                            \n",
       "  Intelâ€¯Xeonâ€¯E5â€‘2690    4         native     375â€¯s                  0.60                  6.2â€¯GB    \n",
       "  Intelâ€¯Xeonâ€¯E5â€‘2690    16        native     244â€¯s                  0.92                  6.2â€¯GB    \n",
       "  Intelâ€¯Xeonâ€¯E5â€‘2690    4         pypdfium   239â€¯s                  0.94                  2.4â€¯GB    \n",
       "  Intelâ€¯Xeonâ€¯E5â€‘2690    16        pypdfium   143â€¯s                  1.57                  2.4â€¯GB    \n",
       "                                                                                                    \n",
       "\n",
       "<span style=\"font-style: italic\">OCR was disabled for these measurements.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Applications</span>\n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Enterprise document search, passage retrieval, and classification.                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Retrievalâ€‘augmented generation (RAG) pipelines (via the companion <span style=\"font-weight: bold\">quackling</span> library).            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Largeâ€‘scale dataset creation, especially for tableâ€‘rich knowledge bases.                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Integration with IBM Data Prep Kit for multimodal training data pipelines.                       \n",
       "\n",
       "<span style=\"font-weight: bold\">Future Work &amp; Community Contributions</span>\n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Add specialized models (figure classification, equation recognition, code detection).            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Improve GPU acceleration and the native PDF backend.                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Encourage openâ€‘source contributions; the project follows an MIT license and includes contribution\n",
       "\n",
       "<span style=\"font-weight: bold\">Takeâ€‘away</span>\n",
       "\n",
       "Docling provides a productionâ€‘ready, locally runnable solution for highâ€‘quality PDF conversion, leveraging stateâ€‘ofâ€‘theâ€‘art layout and table models. Its modular design, fast CPU performance, and extensibility make it a solid\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSummary of â€œDocling Technical Reportâ€ (arXivâ€¯2408.09869)\u001b[0m\n",
       "\n",
       "\u001b[1mWhat is Docling?\u001b[0m\n",
       "Docling is an openâ€‘source (MITâ€‘licensed) Python library for converting PDF documents into structured, machineâ€‘readable formats (JSON or Markdown). It runs locally on commodity hardware and is designed to be fast, stable, and easy to extend.\n",
       "\n",
       "\u001b[1mKey Capabilities\u001b[0m\n",
       "\n",
       "                                                                                                    \n",
       " \u001b[1m \u001b[0m\u001b[1mFeature\u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDescription\u001b[0m\u001b[1m                                                       \u001b[0m\u001b[1m \u001b[0m \n",
       " â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \n",
       "  \u001b[1mLayout analysis\u001b[0m               Uses a custom objectâ€‘detector (RTâ€‘DETRâ€‘based) trained on the        \n",
       "                                large, humanâ€‘annotated DocLayNet dataset to identify page elements  \n",
       "                                (paragraphs, headings, lists, figures, tables, etc.).               \n",
       "  \u001b[1mTable structure recognition\u001b[0m   Employs TableFormer, a visionâ€‘transformer model that recovers       \n",
       "                                logical row/column structures, spanning cells, headers, and         \n",
       "                                hierarchies.                                                        \n",
       "  \u001b[1mOCR (optional)\u001b[0m                Integrates EasyOCR for scanned PDFs (highâ€‘resolution 216â€¯dpi        \n",
       "                                input).                                                             \n",
       "  \u001b[1mMetadata extraction\u001b[0m           Detects title, authors, references, language, and other             \n",
       "                                bibliographic information.                                          \n",
       "  \u001b[1mConfigurable pipelines\u001b[0m        Users can enable/disable OCR, table recognition, set thread         \n",
       "                                budgets, and choose GPU/MPS acceleration.                           \n",
       "  \u001b[1mExtensibility\u001b[0m                 The model pipeline is modular; developers can subclass the base     \n",
       "                                pipeline to add new models (e.g., figure classifier, equation       \n",
       "                                recognizer).                                                        \n",
       "  \u001b[1mBatch & interactive modes\u001b[0m     Optimized for highâ€‘throughput batch processing or lowâ€‘latency       \n",
       "                                interactive use.                                                    \n",
       "                                                                                                    \n",
       "\n",
       "\u001b[1mProcessing Pipeline\u001b[0m\n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mPDF backend\u001b[0m â€“ extracts text tokens with coordinates and renders page images. Two backends are off\n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mAI model stage\u001b[0m â€“ runs layout analysis and tableâ€‘structure models on each page image.             \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1mPostâ€‘processing\u001b[0m â€“ merges predictions, fixes reading order, adds language detection, and assembles\n",
       "\u001b[1;33m 4 \u001b[0m\u001b[1mSerialization\u001b[0m â€“ outputs JSON or Markdown.                                                        \n",
       "\n",
       "\u001b[1mPerformance (reference hardware)\u001b[0m\n",
       "\n",
       "                                                                                                    \n",
       " \u001b[1m                     \u001b[0m \u001b[1m         \u001b[0m \u001b[1m          \u001b[0m \u001b[1m \u001b[0m\u001b[1mTimeâ€‘toâ€‘solution \u001b[0m\u001b[1m   \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mThroughput \u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m \u001b[1m          \u001b[0m \n",
       " \u001b[1m \u001b[0m\u001b[1mSystem\u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mThreads\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mBackend\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m(225â€¯pages)\u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m(pages/s)\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPeak RAM\u001b[0m\u001b[1m \u001b[0m \n",
       " â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \n",
       "  Appleâ€¯M3â€¯Max (16â€¯     4         native     177â€¯s                  1.27                  6.2â€¯GB    \n",
       "  cores)                                                                                            \n",
       "  Appleâ€¯M3â€¯Max (16â€¯     16        native     167â€¯s                  1.34                  6.2â€¯GB    \n",
       "  cores)                                                                                            \n",
       "  Appleâ€¯M3â€¯Max (16â€¯     4         pypdfium   103â€¯s                  2.18                  2.6â€¯GB    \n",
       "  cores)                                                                                            \n",
       "  Appleâ€¯M3â€¯Max (16â€¯     16        pypdfium   92â€¯s                   2.45                  2.6â€¯GB    \n",
       "  cores)                                                                                            \n",
       "  Intelâ€¯Xeonâ€¯E5â€‘2690    4         native     375â€¯s                  0.60                  6.2â€¯GB    \n",
       "  Intelâ€¯Xeonâ€¯E5â€‘2690    16        native     244â€¯s                  0.92                  6.2â€¯GB    \n",
       "  Intelâ€¯Xeonâ€¯E5â€‘2690    4         pypdfium   239â€¯s                  0.94                  2.4â€¯GB    \n",
       "  Intelâ€¯Xeonâ€¯E5â€‘2690    16        pypdfium   143â€¯s                  1.57                  2.4â€¯GB    \n",
       "                                                                                                    \n",
       "\n",
       "\u001b[3mOCR was disabled for these measurements.\u001b[0m\n",
       "\n",
       "\u001b[1mApplications\u001b[0m\n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0mEnterprise document search, passage retrieval, and classification.                               \n",
       "\u001b[1;33m â€¢ \u001b[0mRetrievalâ€‘augmented generation (RAG) pipelines (via the companion \u001b[1mquackling\u001b[0m library).            \n",
       "\u001b[1;33m â€¢ \u001b[0mLargeâ€‘scale dataset creation, especially for tableâ€‘rich knowledge bases.                         \n",
       "\u001b[1;33m â€¢ \u001b[0mIntegration with IBM Data Prep Kit for multimodal training data pipelines.                       \n",
       "\n",
       "\u001b[1mFuture Work & Community Contributions\u001b[0m\n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0mAdd specialized models (figure classification, equation recognition, code detection).            \n",
       "\u001b[1;33m â€¢ \u001b[0mImprove GPU acceleration and the native PDF backend.                                             \n",
       "\u001b[1;33m â€¢ \u001b[0mEncourage openâ€‘source contributions; the project follows an MIT license and includes contribution\n",
       "\n",
       "\u001b[1mTakeâ€‘away\u001b[0m\n",
       "\n",
       "Docling provides a productionâ€‘ready, locally runnable solution for highâ€‘quality PDF conversion, leveraging stateâ€‘ofâ€‘theâ€‘art layout and table models. Its modular design, fast CPU performance, and extensibility make it a solid\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await run_agent(\n",
    "    queries=[\n",
    "        \"Convert the document on https://arxiv.org/pdf/2408.09869 and give me a summary of the document from its markdown content.\",\n",
    "    ],\n",
    "    extra_tools=[],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f0359",
   "metadata": {},
   "source": [
    "### Agentic RAG\n",
    "\n",
    "For large document, or in general for a corpus of documents, we can leverage the power of the Docling document conversion with Agentic RAG.\n",
    "During the ingestion, thanks to Docling, we will ingest the textual document components as well as the more complex structures like tables and figures.\n",
    "While the Agentic RAG enhance the usual RAG flows thanks to 1) having the model rephrasing the retrieval query and 2) allowing the reasoning loop to decide if the retrieved context are good and enough or a second or third retrieval iteration is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f98c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcp.client.streamable_http:Received session ID: 67912c5851164b08ba8851209b50dc49\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-06-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created session_id=6926a2df-c96f-4deb-96ea-1e82fe75a2c2 for Agent(Document Agent)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ðŸ‘¤ User&gt; You are an assistant that uses external tools.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Follow these rules strictly:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\">. At each reasoning step, you may call **at most one tool**.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">. If you need to use multiple tools, do it sequentially:  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Call exactly one tool.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Wait for its result.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Then think again and, if needed, call another tool.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\">. Never call two or more tools in the same response.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\">. If no tool is needed, just provide your reasoning or final answer.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\">. Treat a missing tool result as blocking â€” do not continue reasoning until you receive the output.</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">If you ever attempt to call more than one tool at once, your answer will be rejected.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Always restrict yourself to **zero or one tool call per response**.</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">When the tools return a base64 image, simply create some markdown wrapper which allows to display it and take special care in not hallucinating any byte.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mðŸ‘¤ User> You are an assistant that uses external tools.  \u001b[0m\n",
       "\u001b[36mFollow these rules strictly:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[36m. At each reasoning step, you may call **at most one tool**.  \u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[36m. If you need to use multiple tools, do it sequentially:  \u001b[0m\n",
       "\u001b[36m   - Call exactly one tool.  \u001b[0m\n",
       "\u001b[36m   - Wait for its result.  \u001b[0m\n",
       "\u001b[36m   - Then think again and, if needed, call another tool.  \u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[36m. Never call two or more tools in the same response.  \u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[36m. If no tool is needed, just provide your reasoning or final answer.  \u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[36m. Treat a missing tool result as blocking â€” do not continue reasoning until you receive the output.\u001b[0m\n",
       "\n",
       "\u001b[36mIf you ever attempt to call more than one tool at once, your answer will be rejected.  \u001b[0m\n",
       "\u001b[36mAlways restrict yourself to **zero or one tool call per response**.\u001b[0m\n",
       "\n",
       "\u001b[36mWhen the tools return a base64 image, simply create some markdown wrapper which allows to display it and take special care in not hallucinating any byte.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Agent updated: Document Agent\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Agent updated: Document Agent\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸ¤– Assistant&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸ¤– Assistant>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Understood. I will follow the toolâ€‘usage rules you outlined. Let me know what youâ€™d like me to do!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Understood. I will follow the toolâ€‘usage rules you outlined. Let me know what youâ€™d like me to do!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ðŸ‘¤ User&gt; Ingest the document </span><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://arxiv.org/pdf/2206.01062</span><span style=\"color: #008080; text-decoration-color: #008080\"> into the vectordb vs_</span><span style=\"color: #008080; text-decoration-color: #008080\">dd7afa29-2360-44f8-a329-f51522c425b4</span><span style=\"color: #008080; text-decoration-color: #008080\"> and answer the following questions: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\"> how many pages were manually annotated?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mðŸ‘¤ User> Ingest the document \u001b[0m\u001b[4;36mhttps://arxiv.org/pdf/2206.01062\u001b[0m\u001b[36m into the vectordb vs_\u001b[0m\u001b[36mdd7afa29-2360-44f8-a329-f51522c425b4\u001b[0m\u001b[36m and answer the following questions: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m how many pages were manually annotated?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Agent updated: Document Agent\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Agent updated: Document Agent\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">convert_document_into_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"source\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"https://arxiv.org/pdf/2206.01062\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mconvert_document_into_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"source\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"https://arxiv.org/pdf/2206.01062\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"from_cache\\\": false,\\n  \\\"document_key\\\": \\\"868f49ae1f0e66e82238a8aea43fd30b\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"from_cache\\\": false,\\n  \\\"document_key\\\": \\\"868f49ae1f0e66e82238a8aea43fd30b\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">insert_document_to_vectordb({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"868f49ae1f0e66e82238a8aea43fd30b\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"vector_db_id\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"vs_dd7afa29-2360-44f8-a329-f51522c425b4\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33minsert_document_to_vectordb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"868f49ae1f0e66e82238a8aea43fd30b\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"vector_db_id\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"vs_dd7afa29-2360-44f8-a329-f51522c425b4\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">export_docling_document_to_markdown({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"868f49ae1f0e66e82238a8aea43fd30b\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"max_size\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2000000</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mexport_docling_document_to_markdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"868f49ae1f0e66e82238a8aea43fd30b\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"max_size\"\u001b[0m\u001b[33m: \u001b[0m\u001b[1;33m2000000\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">is_document_in_local_cache({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"868f49ae1f0e66e82238a8aea43fd30b\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mis_document_in_local_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"868f49ae1f0e66e82238a8aea43fd30b\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">export_docling_document_to_markdown({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"868f49ae1f0e66e82238a8aea43fd30b\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mexport_docling_document_to_markdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"868f49ae1f0e66e82238a8aea43fd30b\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">save_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"868f49ae1f0e66e82238a8aea43fd30b\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33msave_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"868f49ae1f0e66e82238a8aea43fd30b\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">convert_document_into_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"source\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"https://arxiv.org/pdf/2206.01062\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mconvert_document_into_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"source\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"https://arxiv.org/pdf/2206.01062\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"vector_db_id\\\": \\\"vs_dd7afa29-2360-44f8-a329-f51522c425b4\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"vector_db_id\\\": \\\"vs_dd7afa29-2360-44f8-a329-f51522c425b4\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"868f49ae1f0e66e82238a8aea43fd30b\\\",\\n  \\\"markdown\\\": \\\"## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\\\n\\\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\\\n\\\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\\\n\\\\nAhmed S. Nassar IBM Research\\\\n\\\\nRueschlikon, Switzerland ahn@zurich.ibm.com\\\\n\\\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\\\n\\\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\\\n\\\\nFigure 1: Four examples of complex page layouts across different document categories\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\n## KEYWORDS\\\\n\\\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning\\\\n\\\\n## ACMReference Format:\\\\n\\\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\\\n\\\\n## ABSTRACT\\\\n\\\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\\\\n\\\\n## CCS CONCEPTS\\\\n\\\\nÂ· Informationsystems â†’ Documentstructure ; Â· Appliedcomputing â†’ Document analysis ; Â· Computing methodologies â†’ Machine learning ; Computer vision ; Object detection ;\\\\n\\\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\\\n\\\\nÂ© 2022 Copyright held by the owner/author(s).\\\\n\\\\nACM ISBN 978-1-4503-9385-0/22/08.\\\\n\\\\nhttps://doi.org/10.1145/3534678.3539043\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\n## 1 INTRODUCTION\\\\n\\\\nDespite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.\\\\n\\\\nAkeyproblem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.\\\\n\\\\nIn this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:\\\\n\\\\n- (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.\\\\n- (2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.\\\\n- (3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.\\\\n- (4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.\\\\n\\\\n1 https://developer.ibm.com/exchanges/data/all/doclaynet\\\\n\\\\nThis enables experimentation with annotation uncertainty and quality control analysis.\\\\n\\\\n- (5) Pre-defined Train-, Test- &amp;amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp;amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.\\\\n\\\\nAll aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.\\\\n\\\\nIn Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.\\\\n\\\\n## 2 RELATED WORK\\\\n\\\\nWhile early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].\\\\n\\\\nLately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.\\\\n\\\\n## 3 THE DOCLAYNET DATASET\\\\n\\\\nDocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula , List-item , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.\\\\n\\\\nIn addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents\\\\n\\\\nFigure 2: Distribution of DocLayNet pages across document categories.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nto a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents ( &amp;gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing 'text in the wild\\\\\\\".\\\\n\\\\nThe pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals , Scientific Articles , Laws &amp;amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories ( Financial Reports and Manuals ) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.\\\\n\\\\nWe did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.\\\\n\\\\nTo ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.\\\\n\\\\n2 e.g. AAPL from https://www.annualreports.com/\\\\n\\\\nTable 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.\\\\n\\\\nIn order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025 Ã— 1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.\\\\n\\\\nDespite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.\\\\n\\\\n## 4 ANNOTATION CAMPAIGN\\\\n\\\\nThe annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,\\\\n\\\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\\\n\\\\n|                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |\\\\n|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\\\\n| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\\\\n| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\\\\n| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\\\\n| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\\\\n| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\\\\n| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\\\\n| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\\\\n| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\\\\n| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\\\\n| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\\\\n| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\\\\n| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\\\\n| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\\\\n\\\\nFigure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nwe distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.\\\\n\\\\nPhase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv 3 , government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.\\\\n\\\\nPreparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.\\\\n\\\\nPhase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula , List-item , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on\\\\n\\\\n3 https://arxiv.org/\\\\n\\\\nthe textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.\\\\n\\\\nAt first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.\\\\n\\\\nObviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:\\\\n\\\\n- (1) Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.\\\\n- (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.\\\\n- (3) For every Caption , there must be exactly one corresponding Picture or Table .\\\\n- (4) Connected sub-pictures are grouped together in one Picture object.\\\\n- (5) Formula numbers are included in a Formula object.\\\\n- (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.\\\\n\\\\nThe complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.\\\\n\\\\nPhase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\n05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0\\\\n\\\\nFigure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.\\\\n\\\\nwere carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.\\\\n\\\\nPhase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted\\\\n\\\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\\\n\\\\n|                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |\\\\n|----------------|---------|---------|---------|---------|--------|\\\\n|                |         | R50     | R101    | R101    | v5x6   |\\\\n| Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |\\\\n| Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |\\\\n| Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |\\\\n| List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |\\\\n| Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |\\\\n| Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |\\\\n| Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |\\\\n| Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |\\\\n| Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |\\\\n| Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |\\\\n| Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |\\\\n| All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |\\\\n\\\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\\\n\\\\n## 5 EXPERIMENTS\\\\n\\\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\\\n\\\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\\\n\\\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\\\n\\\\nTable 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.\\\\n\\\\n| Class-count    |   11 | 6       | 5       | 4       |\\\\n|----------------|------|---------|---------|---------|\\\\n| Caption        |   68 | Text    | Text    | Text    |\\\\n| Footnote       |   71 | Text    | Text    | Text    |\\\\n| Formula        |   60 | Text    | Text    | Text    |\\\\n| List-item      |   81 | Text    | 82      | Text    |\\\\n| Page-footer    |   62 | 62      | -       | -       |\\\\n| Page-header    |   72 | 68      | -       | -       |\\\\n| Picture        |   72 | 72      | 72      | 72      |\\\\n| Section-header |   68 | 67      | 69      | 68      |\\\\n| Table          |   82 | 83      | 82      | 82      |\\\\n| Text           |   85 | 84      | 84      | 84      |\\\\n| Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |\\\\n| Overall        |   72 | 73      | 78      | 77      |\\\\n\\\\n## Learning Curve\\\\n\\\\nOne of the fundamental questions related to any dataset is if it is 'large enough'. To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.\\\\n\\\\n## Impact of Class Labels\\\\n\\\\nThe choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption â†’ Text ) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of\\\\n\\\\nTable 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in /tildelow 10% point improvement.\\\\n\\\\n| Class-count Split   | 11   | 11   | 5   | 5    |\\\\n|---------------------|------|------|-----|------|\\\\n|                     | Doc  | Page | Doc | Page |\\\\n| Caption             | 68   | 83   |     |      |\\\\n| Footnote            | 71   | 84   |     |      |\\\\n| Formula             | 60   | 66   |     |      |\\\\n| List-item           | 81   | 88   | 82  | 88   |\\\\n| Page-footer         | 62   | 89   |     |      |\\\\n| Page-header         | 72   | 90   |     |      |\\\\n| Picture             | 72   | 82   | 72  | 82   |\\\\n| Section-header      | 68   | 83   | 69  | 83   |\\\\n| Table               | 82   | 89   | 82  | 90   |\\\\n| Text                | 85   | 91   | 84  | 90   |\\\\n| Title               | 77   | 81   |     |      |\\\\n| All                 | 72   | 84   | 78  | 87   |\\\\n\\\\nlists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.\\\\n\\\\n## Impact of Document Split in Train and Test Set\\\\n\\\\nMany documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains Ëœ 10% in mAP over the document-wise splitting. Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.\\\\n\\\\n## Dataset Comparison\\\\n\\\\nThroughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp;amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.\\\\n\\\\n|                 |            | Testing on   | Testing on   | Testing on   |\\\\n|-----------------|------------|--------------|--------------|--------------|\\\\n| Training on     | labels     | PLN          | DB           | DLN          |\\\\n| PubLayNet (PLN) | Figure     | 96           | 43           | 23           |\\\\n| PubLayNet (PLN) | Sec-header | 87           | -            | 32           |\\\\n|                 | Table      | 95           | 24           | 49           |\\\\n|                 | Text       | 96           | -            | 42           |\\\\n|                 | total      | 93           | 34           | 30           |\\\\n| DocBank (DB)    | Figure     | 77           | 71           | 31           |\\\\n| DocBank (DB)    | Table      | 19           | 65           | 22           |\\\\n| DocBank (DB)    | total      | 48           | 68           | 27           |\\\\n| DocLayNet (DLN) | Figure     | 67           | 51           | 72           |\\\\n| DocLayNet (DLN) | Sec-header | 53           | -            | 68           |\\\\n|                 | Table      | 87           | 43           | 82           |\\\\n|                 | Text       | 77           | -            | 84           |\\\\n|                 | total      | 59           | 47           | 78           |\\\\n\\\\nSection-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text .\\\\n\\\\nFor comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.\\\\n\\\\n## Example Predictions\\\\n\\\\nTo conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.\\\\n\\\\n## 6 CONCLUSION\\\\n\\\\nIn this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.\\\\n\\\\nFrom the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.\\\\n\\\\nTo date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.\\\\n\\\\n## REFERENCES\\\\n\\\\n- [1] Max GÃ¶bel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.\\\\n- [2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1404-1410, 2017.\\\\n- [3] HervÃ© DÃ©jean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.\\\\n- [4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.\\\\n- [5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR) , pages 1-11, 01 2022.\\\\n- [6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.\\\\n- [7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.\\\\n- [8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.\\\\n- [9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.\\\\n- [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.\\\\n- [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.\\\\n- [12] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.\\\\n- [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu\\\\n\\\\nFigure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nDiaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.\\\\n\\\\n- [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.\\\\n- [15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.\\\\n- [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.\\\\n- [17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.\\\\n- [18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.\\\\n- [19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.\\\\n- [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.\\\\n- [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.\\\\n- [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.\\\\n- [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6(1):60, 2019.\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"868f49ae1f0e66e82238a8aea43fd30b\\\",\\n  \\\"markdown\\\": \\\"## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\\\n\\\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\\\n\\\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\\\n\\\\nAhmed S. Nassar IBM Research\\\\n\\\\nRueschlikon, Switzerland ahn@zurich.ibm.com\\\\n\\\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\\\n\\\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\\\n\\\\nFigure 1: Four examples of complex page layouts across different document categories\\\\n\\\\n\u001b[0m\u001b[33m<\u001b[0m\u001b[33m!-- image -->\\\\n\\\\n## KEYWORDS\\\\n\\\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning\\\\n\\\\n## ACMReference Format:\\\\n\\\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining \u001b[0m\u001b[33m(\u001b[0m\u001b[33mKDD '22\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\\\n\\\\n## ABSTRACT\\\\n\\\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores \u001b[0m\u001b[33m(\u001b[0m\u001b[33min mAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\\\\n\\\\n## CCS CONCEPTS\\\\n\\\\nÂ· Informationsystems â†’ Documentstructure ; Â· Appliedcomputing â†’ Document analysis ; Â· Computing methodologies â†’ Machine learning ; Computer vision ; Object detection ;\\\\n\\\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author\u001b[0m\u001b[33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\\\n\\\\nÂ© 2022 Copyright held by the owner/author\u001b[0m\u001b[33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nACM ISBN 978-1-4503-9385-0/22/08.\\\\n\\\\nhttps://doi.org/10.1145/3534678.3539043\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\n## 1 INTRODUCTION\\\\n\\\\nDespite the substantial improvements achieved with machine-learning \u001b[0m\u001b[33m(\u001b[0m\u001b[33mML\u001b[0m\u001b[33m)\u001b[0m\u001b[33m approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1-4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. The challenge originates from the huge variability in PDF documents regarding layout, language and formats \u001b[0m\u001b[33m(\u001b[0m\u001b[33mscanned, programmatic or a combination of both\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.\\\\n\\\\nAkeyproblem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and DocBank \u001b[0m\u001b[33m[\u001b[0m\u001b[33m7\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories \u001b[0m\u001b[33m(\u001b[0m\u001b[33mPubMed and arXiv\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.\\\\n\\\\nIn this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:\\\\n\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m2\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m3\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m4\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.\\\\n\\\\n1 https://developer.ibm.com/exchanges/data/all/doclaynet\\\\n\\\\nThis enables experimentation with annotation uncertainty and quality control analysis.\\\\n\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m5\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Pre-defined Train-, Test- &amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.\\\\n\\\\nAll aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.\\\\n\\\\nIn Section 5, we will present baseline accuracy numbers for a variety of object detection methods \u001b[0m\u001b[33m(\u001b[0m\u001b[33mFaster R-CNN, Mask R-CNN and YOLOv5\u001b[0m\u001b[33m)\u001b[0m\u001b[33m trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.\\\\n\\\\n## 2 RELATED WORK\\\\n\\\\nWhile early approaches in document-layout analysis used rulebased algorithms and heuristics \u001b[0m\u001b[33m[\u001b[0m\u001b[33m8\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models \u001b[0m\u001b[33m[\u001b[0m\u001b[33m9-15\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and common deep-learning frameworks \u001b[0m\u001b[33m[\u001b[0m\u001b[33m17\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Reference data sets such as PubLayNet \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and DocBank provide their data in the commonly accepted COCO format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\nLately, new types of ML models for document-layout analysis have emerged in the community \u001b[0m\u001b[33m[\u001b[0m\u001b[33m18-21\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.\\\\n\\\\n## 3 THE DOCLAYNET DATASET\\\\n\\\\nDocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula , List-item , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.\\\\n\\\\nIn addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents\\\\n\\\\nFigure 2: Distribution of DocLayNet pages across document categories.\\\\n\\\\n<!-- image -->\\\\n\\\\nto a minimum, since they introduce difficulties in annotation \u001b[0m\u001b[33m(\u001b[0m\u001b[33msee Section 4\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. As a second condition, we focussed on medium to large documents \u001b[0m\u001b[33m(\u001b[0m\u001b[33m &gt; 10 pages\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing 'text in the wild\\\\\\\".\\\\n\\\\nThe pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals , Scientific Articles , Laws &amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories \u001b[0m\u001b[33m(\u001b[0m\u001b[33m Financial Reports and Manuals \u001b[0m\u001b[33m)\u001b[0m\u001b[33m contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.\\\\n\\\\nWe did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mclose to 95%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m are published in English language. However, DocLayNet also contains a number of documents in other languages such as German \u001b[0m\u001b[33m(\u001b[0m\u001b[33m2.5%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, French \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1.0%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and Japanese \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1.0%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.\\\\n\\\\nTo ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.\\\\n\\\\n2 e.g. AAPL from https://www.annualreports.com/\\\\n\\\\nTable 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.\\\\n\\\\nIn order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. This entails the standard COCO ground-truth file \u001b[0m\u001b[33m(\u001b[0m\u001b[33min JSON format\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with the associated page images \u001b[0m\u001b[33m(\u001b[0m\u001b[33min PNG format, 1025 Ã— 1025 pixels\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates \u001b[0m\u001b[33m(\u001b[0m\u001b[33min JSON\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. All additional files are linked to the primary page images by their matching filenames.\\\\n\\\\nDespite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting \u001b[0m\u001b[33m(\u001b[0m\u001b[33msee Table 1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.\\\\n\\\\n## 4 ANNOTATION CAMPAIGN\\\\n\\\\nThe annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,\\\\n\\\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence \u001b[0m\u001b[33m(\u001b[0m\u001b[33mas % of row 'Total'\u001b[0m\u001b[33m)\u001b[0m\u001b[33m in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\\\n\\\\n|                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   |\\\\n|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\\\\n| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\\\\n| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\\\\n| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\\\\n| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\\\\n| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\\\\n| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\\\\n| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\\\\n| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\\\\n| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\\\\n| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\\\\n| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\\\\n| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\\\\n| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\\\\n\\\\nFigure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells \u001b[0m\u001b[33m(\u001b[0m\u001b[33min darker shades\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.\\\\n\\\\n<!-- image -->\\\\n\\\\nwe distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.\\\\n\\\\nPhase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv 3 , government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.\\\\n\\\\nPreparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service \u001b[0m\u001b[33m(\u001b[0m\u001b[33mCCS\u001b[0m\u001b[33m)\u001b[0m\u001b[33m \u001b[0m\u001b[33m[\u001b[0m\u001b[33m22\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.\\\\n\\\\nPhase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula , List-item , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m the overall occurrence of the label, \u001b[0m\u001b[33m(\u001b[0m\u001b[33m2\u001b[0m\u001b[33m)\u001b[0m\u001b[33m the specificity of the label, \u001b[0m\u001b[33m(\u001b[0m\u001b[33m3\u001b[0m\u001b[33m)\u001b[0m\u001b[33m recognisability on a single page \u001b[0m\u001b[33m(\u001b[0m\u001b[33mi.e. no need for context from previous or next page\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and \u001b[0m\u001b[33m(\u001b[0m\u001b[33m4\u001b[0m\u001b[33m)\u001b[0m\u001b[33m overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on\\\\n\\\\n3 https://arxiv.org/\\\\n\\\\nthe textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.\\\\n\\\\nAt first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.\\\\n\\\\nObviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:\\\\n\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m2\u001b[0m\u001b[33m)\u001b[0m\u001b[33m A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m3\u001b[0m\u001b[33m)\u001b[0m\u001b[33m For every Caption , there must be exactly one corresponding Picture or Table .\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m4\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Connected sub-pictures are grouped together in one Picture object.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m5\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Formula numbers are included in a Formula object.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m6\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Emphasised text \u001b[0m\u001b[33m(\u001b[0m\u001b[33me.g. in italic or bold\u001b[0m\u001b[33m)\u001b[0m\u001b[33m at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.\\\\n\\\\nThe complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.\\\\n\\\\nPhase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets \u001b[0m\u001b[33m(\u001b[0m\u001b[33mblinded from the reference\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations\\\\n\\\\n<!-- image -->\\\\n\\\\n05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0\\\\n\\\\nFigure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.\\\\n\\\\nwere carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.\\\\n\\\\nPhase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement \u001b[0m\u001b[33m(\u001b[0m\u001b[33msee Table 1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. We wanted\\\\n\\\\nTable 2: Prediction performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of object detection networks on DocLayNet test set. The MRCNN \u001b[0m\u001b[33m(\u001b[0m\u001b[33mMask R-CNN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and FRCNN \u001b[0m\u001b[33m(\u001b[0m\u001b[33mFaster R-CNN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo \u001b[0m\u001b[33m(\u001b[0m\u001b[33mMask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, with default configurations. The YOLO implementation utilized was YOLOv5x6 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\\\n\\\\n|                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |\\\\n|----------------|---------|---------|---------|---------|--------|\\\\n|                |         | R50     | R101    | R101    | v5x6   |\\\\n| Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |\\\\n| Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |\\\\n| Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |\\\\n| List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |\\\\n| Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |\\\\n| Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |\\\\n| Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |\\\\n| Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |\\\\n| Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |\\\\n| Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |\\\\n| Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |\\\\n| All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |\\\\n\\\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\\\n\\\\n## 5 EXPERIMENTS\\\\n\\\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and the availability of general frameworks such as detectron2 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m17\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\\\n\\\\nFigure 5: Prediction performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\\\n\\\\n<!-- image -->\\\\n\\\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\\\n\\\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. These scores are computed by leveraging the evaluation code provided by the COCO API \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nIn Table 2, we present baseline experiments \u001b[0m\u001b[33m(\u001b[0m\u001b[33mgiven in mAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m on Mask R-CNN \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, Faster R-CNN \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, and YOLOv5 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\\\n\\\\nTable 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.\\\\n\\\\n| Class-count    |   11 | 6       | 5       | 4       |\\\\n|----------------|------|---------|---------|---------|\\\\n| Caption        |   68 | Text    | Text    | Text    |\\\\n| Footnote       |   71 | Text    | Text    | Text    |\\\\n| Formula        |   60 | Text    | Text    | Text    |\\\\n| List-item      |   81 | Text    | 82      | Text    |\\\\n| Page-footer    |   62 | 62      | -       | -       |\\\\n| Page-header    |   72 | 68      | -       | -       |\\\\n| Picture        |   72 | 72      | 72      | 72      |\\\\n| Section-header |   68 | 67      | 69      | 68      |\\\\n| Table          |   82 | 83      | 82      | 82      |\\\\n| Text           |   85 | 84      | 84      | 84      |\\\\n| Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |\\\\n| Overall        |   72 | 73      | 78      | 77      |\\\\n\\\\n## Learning Curve\\\\n\\\\nOne of the fundamental questions related to any dataset is if it is 'large enough'. To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency \u001b[0m\u001b[33m(\u001b[0m\u001b[33mas discussed in Section 3\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, data augmentation methods \u001b[0m\u001b[33m[\u001b[0m\u001b[33m23\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, or the addition of more document categories and styles.\\\\n\\\\n## Impact of Class Labels\\\\n\\\\nThe choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones \u001b[0m\u001b[33m(\u001b[0m\u001b[33me.g. Caption â†’ Text \u001b[0m\u001b[33m)\u001b[0m\u001b[33m or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of\\\\n\\\\nTable 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in /tildelow 10% point improvement.\\\\n\\\\n| Class-count Split   | 11   | 11   | 5   | 5    |\\\\n|---------------------|------|------|-----|------|\\\\n|                     | Doc  | Page | Doc | Page |\\\\n| Caption             | 68   | 83   |     |      |\\\\n| Footnote            | 71   | 84   |     |      |\\\\n| Formula             | 60   | 66   |     |      |\\\\n| List-item           | 81   | 88   | 82  | 88   |\\\\n| Page-footer         | 62   | 89   |     |      |\\\\n| Page-header         | 72   | 90   |     |      |\\\\n| Picture             | 72   | 82   | 72  | 82   |\\\\n| Section-header      | 68   | 83   | 69  | 83   |\\\\n| Table               | 82   | 89   | 82  | 90   |\\\\n| Text                | 85   | 91   | 84  | 90   |\\\\n| Title               | 77   | 81   |     |      |\\\\n| All                 | 72   | 84   | 78  | 87   |\\\\n\\\\nlists in PubLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mgrouped list-items\u001b[0m\u001b[33m)\u001b[0m\u001b[33m versus DocLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mseparate list-items\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.\\\\n\\\\n## Impact of Document Split in Train and Test Set\\\\n\\\\nMany documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains Ëœ 10% in mAP over the document-wise splitting. Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.\\\\n\\\\n## Dataset Comparison\\\\n\\\\nThroughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets \u001b[0m\u001b[33m(\u001b[0m\u001b[33mPubLayNet, DocBank and DocLayNet\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 5: Prediction Performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.\\\\n\\\\n|                 |            | Testing on   | Testing on   | Testing on   |\\\\n|-----------------|------------|--------------|--------------|--------------|\\\\n| Training on     | labels     | PLN          | DB           | DLN          |\\\\n| PubLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mPLN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | Figure     | 96           | 43           | 23           |\\\\n| PubLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mPLN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | Sec-header | 87           | -            | 32           |\\\\n|                 | Table      | 95           | 24           | 49           |\\\\n|                 | Text       | 96           | -            | 42           |\\\\n|                 | total      | 93           | 34           | 30           |\\\\n| DocBank \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    | Figure     | 77           | 71           | 31           |\\\\n| DocBank \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    | Table      | 19           | 65           | 22           |\\\\n| DocBank \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    | total      | 48           | 68           | 27           |\\\\n| DocLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDLN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | Figure     | 67           | 51           | 72           |\\\\n| DocLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDLN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | Sec-header | 53           | -            | 68           |\\\\n|                 | Table      | 87           | 43           | 82           |\\\\n|                 | Text       | 77           | -            | 84           |\\\\n|                 | total      | 59           | 47           | 78           |\\\\n\\\\nSection-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists \u001b[0m\u001b[33m(\u001b[0m\u001b[33mby list-element vs. whole list objects\u001b[0m\u001b[33m)\u001b[0m\u001b[33m naturally decreases the mAP score for Text .\\\\n\\\\nFor comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.\\\\n\\\\n## Example Predictions\\\\n\\\\nTo conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.\\\\n\\\\n## 6 CONCLUSION\\\\n\\\\nIn this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.\\\\n\\\\nFrom the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mthrough double and triple annotations\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.\\\\n\\\\nTo date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.\\\\n\\\\n## REFERENCES\\\\n\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Max GÃ¶bel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m2\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition \u001b[0m\u001b[33m(\u001b[0m\u001b[33mICDAR\u001b[0m\u001b[33m)\u001b[0m\u001b[33m , volume 01, pages 1404-1410, 2017.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m3\u001b[0m\u001b[33m]\u001b[0m\u001b[33m HervÃ© DÃ©jean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition \u001b[0m\u001b[33m(\u001b[0m\u001b[33mcTDaR\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, April 2019. http://sac.founderit.com/.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition \u001b[0m\u001b[33m(\u001b[0m\u001b[33mIJDAR\u001b[0m\u001b[33m)\u001b[0m\u001b[33m , pages 1-11, 01 2022.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m7\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m8\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m10\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39\u001b[0m\u001b[33m(\u001b[0m\u001b[33m6\u001b[0m\u001b[33m)\u001b[0m\u001b[33m:1137-1149, 2017.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu\\\\n\\\\nFigure 6: Example layout predictions on selected pages from the DocLayNet test-set. \u001b[0m\u001b[33m(\u001b[0m\u001b[33mA, D\u001b[0m\u001b[33m)\u001b[0m\u001b[33m exhibit favourable results on coloured backgrounds. \u001b[0m\u001b[33m(\u001b[0m\u001b[33mB, C\u001b[0m\u001b[33m)\u001b[0m\u001b[33m show accurate list-item and paragraph differentiation despite densely-spaced lines. \u001b[0m\u001b[33m(\u001b[0m\u001b[33mE\u001b[0m\u001b[33m)\u001b[0m\u001b[33m demonstrates good table and figure distinction. \u001b[0m\u001b[33m(\u001b[0m\u001b[33mF\u001b[0m\u001b[33m)\u001b[0m\u001b[33m shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.\\\\n\\\\n<!-- image --\u001b[0m\u001b[33m>\u001b[0m\u001b[33m\\\\n\\\\nDiaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.\\\\n\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m14\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m15\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m17\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m18\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m19\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m20\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m21\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m22\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m23\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6\u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m:60, 2019.\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"in_cache\\\": true\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"in_cache\\\": true\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"868f49ae1f0e66e82238a8aea43fd30b\\\",\\n  \\\"markdown\\\": \\\"## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\\\n\\\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\\\n\\\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\\\n\\\\nAhmed S. Nassar IBM Research\\\\n\\\\nRueschlikon, Switzerland ahn@zurich.ibm.com\\\\n\\\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\\\n\\\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\\\n\\\\nFigure 1: Four examples of complex page layouts across different document categories\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\n## KEYWORDS\\\\n\\\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning\\\\n\\\\n## ACMReference Format:\\\\n\\\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\\\n\\\\n## ABSTRACT\\\\n\\\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\\\\n\\\\n## CCS CONCEPTS\\\\n\\\\nÂ· Informationsystems â†’ Documentstructure ; Â· Appliedcomputing â†’ Document analysis ; Â· Computing methodologies â†’ Machine learning ; Computer vision ; Object detection ;\\\\n\\\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\\\n\\\\nÂ© 2022 Copyright held by the owner/author(s).\\\\n\\\\nACM ISBN 978-1-4503-9385-0/22/08.\\\\n\\\\nhttps://doi.org/10.1145/3534678.3539043\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\n## 1 INTRODUCTION\\\\n\\\\nDespite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.\\\\n\\\\nAkeyproblem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.\\\\n\\\\nIn this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:\\\\n\\\\n- (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.\\\\n- (2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.\\\\n- (3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.\\\\n- (4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.\\\\n\\\\n1 https://developer.ibm.com/exchanges/data/all/doclaynet\\\\n\\\\nThis enables experimentation with annotation uncertainty and quality control analysis.\\\\n\\\\n- (5) Pre-defined Train-, Test- &amp;amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp;amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.\\\\n\\\\nAll aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.\\\\n\\\\nIn Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.\\\\n\\\\n## 2 RELATED WORK\\\\n\\\\nWhile early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].\\\\n\\\\nLately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.\\\\n\\\\n## 3 THE DOCLAYNET DATASET\\\\n\\\\nDocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula , List-item , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.\\\\n\\\\nIn addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents\\\\n\\\\nFigure 2: Distribution of DocLayNet pages across document categories.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nto a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents ( &amp;gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing 'text in the wild\\\\\\\".\\\\n\\\\nThe pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals , Scientific Articles , Laws &amp;amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories ( Financial Reports and Manuals ) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.\\\\n\\\\nWe did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.\\\\n\\\\nTo ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.\\\\n\\\\n2 e.g. AAPL from https://www.annualreports.com/\\\\n\\\\nTable 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.\\\\n\\\\nIn order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025 Ã— 1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.\\\\n\\\\nDespite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.\\\\n\\\\n## 4 ANNOTATION CAMPAIGN\\\\n\\\\nThe annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,\\\\n\\\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\\\n\\\\n|                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |\\\\n|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\\\\n| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\\\\n| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\\\\n| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\\\\n| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\\\\n| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\\\\n| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\\\\n| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\\\\n| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\\\\n| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\\\\n| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\\\\n| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\\\\n| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\\\\n| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\\\\n\\\\nFigure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nwe distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.\\\\n\\\\nPhase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv 3 , government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.\\\\n\\\\nPreparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.\\\\n\\\\nPhase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula , List-item , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on\\\\n\\\\n3 https://arxiv.org/\\\\n\\\\nthe textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.\\\\n\\\\nAt first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.\\\\n\\\\nObviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:\\\\n\\\\n- (1) Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.\\\\n- (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.\\\\n- (3) For every Caption , there must be exactly one corresponding Picture or Table .\\\\n- (4) Connected sub-pictures are grouped together in one Picture object.\\\\n- (5) Formula numbers are included in a Formula object.\\\\n- (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.\\\\n\\\\nThe complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.\\\\n\\\\nPhase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\n05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0\\\\n\\\\nFigure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.\\\\n\\\\nwere carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.\\\\n\\\\nPhase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted\\\\n\\\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\\\n\\\\n|                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |\\\\n|----------------|---------|---------|---------|---------|--------|\\\\n|                |         | R50     | R101    | R101    | v5x6   |\\\\n| Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |\\\\n| Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |\\\\n| Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |\\\\n| List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |\\\\n| Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |\\\\n| Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |\\\\n| Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |\\\\n| Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |\\\\n| Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |\\\\n| Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |\\\\n| Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |\\\\n| All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |\\\\n\\\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\\\n\\\\n## 5 EXPERIMENTS\\\\n\\\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\\\n\\\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\\\n\\\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\\\n\\\\nTable 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.\\\\n\\\\n| Class-count    |   11 | 6       | 5       | 4       |\\\\n|----------------|------|---------|---------|---------|\\\\n| Caption        |   68 | Text    | Text    | Text    |\\\\n| Footnote       |   71 | Text    | Text    | Text    |\\\\n| Formula        |   60 | Text    | Text    | Text    |\\\\n| List-item      |   81 | Text    | 82      | Text    |\\\\n| Page-footer    |   62 | 62      | -       | -       |\\\\n| Page-header    |   72 | 68      | -       | -       |\\\\n| Picture        |   72 | 72      | 72      | 72      |\\\\n| Section-header |   68 | 67      | 69      | 68      |\\\\n| Table          |   82 | 83      | 82      | 82      |\\\\n| Text           |   85 | 84      | 84      | 84      |\\\\n| Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |\\\\n| Overall        |   72 | 73      | 78      | 77      |\\\\n\\\\n## Learning Curve\\\\n\\\\nOne of the fundamental questions related to any dataset is if it is 'large enough'. To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.\\\\n\\\\n## Impact of Class Labels\\\\n\\\\nThe choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption â†’ Text ) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of\\\\n\\\\nTable 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in /tildelow 10% point improvement.\\\\n\\\\n| Class-count Split   | 11   | 11   | 5   | 5    |\\\\n|---------------------|------|------|-----|------|\\\\n|                     | Doc  | Page | Doc | Page |\\\\n| Caption             | 68   | 83   |     |      |\\\\n| Footnote            | 71   | 84   |     |      |\\\\n| Formula             | 60   | 66   |     |      |\\\\n| List-item           | 81   | 88   | 82  | 88   |\\\\n| Page-footer         | 62   | 89   |     |      |\\\\n| Page-header         | 72   | 90   |     |      |\\\\n| Picture             | 72   | 82   | 72  | 82   |\\\\n| Section-header      | 68   | 83   | 69  | 83   |\\\\n| Table               | 82   | 89   | 82  | 90   |\\\\n| Text                | 85   | 91   | 84  | 90   |\\\\n| Title               | 77   | 81   |     |      |\\\\n| All                 | 72   | 84   | 78  | 87   |\\\\n\\\\nlists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.\\\\n\\\\n## Impact of Document Split in Train and Test Set\\\\n\\\\nMany documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains Ëœ 10% in mAP over the document-wise splitting. Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.\\\\n\\\\n## Dataset Comparison\\\\n\\\\nThroughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp;amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.\\\\n\\\\n|                 |            | Testing on   | Testing on   | Testing on   |\\\\n|-----------------|------------|--------------|--------------|--------------|\\\\n| Training on     | labels     | PLN          | DB           | DLN          |\\\\n| PubLayNet (PLN) | Figure     | 96           | 43           | 23           |\\\\n| PubLayNet (PLN) | Sec-header | 87           | -            | 32           |\\\\n|                 | Table      | 95           | 24           | 49           |\\\\n|                 | Text       | 96           | -            | 42           |\\\\n|                 | total      | 93           | 34           | 30           |\\\\n| DocBank (DB)    | Figure     | 77           | 71           | 31           |\\\\n| DocBank (DB)    | Table      | 19           | 65           | 22           |\\\\n| DocBank (DB)    | total      | 48           | 68           | 27           |\\\\n| DocLayNet (DLN) | Figure     | 67           | 51           | 72           |\\\\n| DocLayNet (DLN) | Sec-header | 53           | -            | 68           |\\\\n|                 | Table      | 87           | 43           | 82           |\\\\n|                 | Text       | 77           | -            | 84           |\\\\n|                 | total      | 59           | 47           | 78           |\\\\n\\\\nSection-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text .\\\\n\\\\nFor comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.\\\\n\\\\n## Example Predictions\\\\n\\\\nTo conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.\\\\n\\\\n## 6 CONCLUSION\\\\n\\\\nIn this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.\\\\n\\\\nFrom the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.\\\\n\\\\nTo date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.\\\\n\\\\n## REFERENCES\\\\n\\\\n- [1] Max GÃ¶bel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.\\\\n- [2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1404-1410, 2017.\\\\n- [3] HervÃ© DÃ©jean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.\\\\n- [4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.\\\\n- [5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR) , pages 1-11, 01 2022.\\\\n- [6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.\\\\n- [7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.\\\\n- [8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.\\\\n- [9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.\\\\n- [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.\\\\n- [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.\\\\n- [12] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.\\\\n- [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu\\\\n\\\\nFigure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.\\\\n\\\\n&lt;!-- image --&gt;\\\\n\\\\nDiaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.\\\\n\\\\n- [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.\\\\n- [15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.\\\\n- [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.\\\\n- [17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.\\\\n- [18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.\\\\n- [19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.\\\\n- [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.\\\\n- [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.\\\\n- [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.\\\\n- [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6(1):60, 2019.\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"868f49ae1f0e66e82238a8aea43fd30b\\\",\\n  \\\"markdown\\\": \\\"## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\\\n\\\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\\\n\\\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\\\n\\\\nAhmed S. Nassar IBM Research\\\\n\\\\nRueschlikon, Switzerland ahn@zurich.ibm.com\\\\n\\\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\\\n\\\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\\\n\\\\nFigure 1: Four examples of complex page layouts across different document categories\\\\n\\\\n\u001b[0m\u001b[33m<\u001b[0m\u001b[33m!-- image -->\\\\n\\\\n## KEYWORDS\\\\n\\\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning\\\\n\\\\n## ACMReference Format:\\\\n\\\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining \u001b[0m\u001b[33m(\u001b[0m\u001b[33mKDD '22\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\\\n\\\\n## ABSTRACT\\\\n\\\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores \u001b[0m\u001b[33m(\u001b[0m\u001b[33min mAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\\\\n\\\\n## CCS CONCEPTS\\\\n\\\\nÂ· Informationsystems â†’ Documentstructure ; Â· Appliedcomputing â†’ Document analysis ; Â· Computing methodologies â†’ Machine learning ; Computer vision ; Object detection ;\\\\n\\\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author\u001b[0m\u001b[33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\\\n\\\\nÂ© 2022 Copyright held by the owner/author\u001b[0m\u001b[33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[33m)\u001b[0m\u001b[33m.\\\\n\\\\nACM ISBN 978-1-4503-9385-0/22/08.\\\\n\\\\nhttps://doi.org/10.1145/3534678.3539043\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\n## 1 INTRODUCTION\\\\n\\\\nDespite the substantial improvements achieved with machine-learning \u001b[0m\u001b[33m(\u001b[0m\u001b[33mML\u001b[0m\u001b[33m)\u001b[0m\u001b[33m approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1-4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. The challenge originates from the huge variability in PDF documents regarding layout, language and formats \u001b[0m\u001b[33m(\u001b[0m\u001b[33mscanned, programmatic or a combination of both\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.\\\\n\\\\nAkeyproblem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and DocBank \u001b[0m\u001b[33m[\u001b[0m\u001b[33m7\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories \u001b[0m\u001b[33m(\u001b[0m\u001b[33mPubMed and arXiv\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.\\\\n\\\\nIn this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:\\\\n\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m2\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m3\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m4\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.\\\\n\\\\n1 https://developer.ibm.com/exchanges/data/all/doclaynet\\\\n\\\\nThis enables experimentation with annotation uncertainty and quality control analysis.\\\\n\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m5\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Pre-defined Train-, Test- &amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.\\\\n\\\\nAll aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.\\\\n\\\\nIn Section 5, we will present baseline accuracy numbers for a variety of object detection methods \u001b[0m\u001b[33m(\u001b[0m\u001b[33mFaster R-CNN, Mask R-CNN and YOLOv5\u001b[0m\u001b[33m)\u001b[0m\u001b[33m trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.\\\\n\\\\n## 2 RELATED WORK\\\\n\\\\nWhile early approaches in document-layout analysis used rulebased algorithms and heuristics \u001b[0m\u001b[33m[\u001b[0m\u001b[33m8\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models \u001b[0m\u001b[33m[\u001b[0m\u001b[33m9-15\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and common deep-learning frameworks \u001b[0m\u001b[33m[\u001b[0m\u001b[33m17\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Reference data sets such as PubLayNet \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and DocBank provide their data in the commonly accepted COCO format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\nLately, new types of ML models for document-layout analysis have emerged in the community \u001b[0m\u001b[33m[\u001b[0m\u001b[33m18-21\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.\\\\n\\\\n## 3 THE DOCLAYNET DATASET\\\\n\\\\nDocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula , List-item , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.\\\\n\\\\nIn addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents\\\\n\\\\nFigure 2: Distribution of DocLayNet pages across document categories.\\\\n\\\\n<!-- image -->\\\\n\\\\nto a minimum, since they introduce difficulties in annotation \u001b[0m\u001b[33m(\u001b[0m\u001b[33msee Section 4\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. As a second condition, we focussed on medium to large documents \u001b[0m\u001b[33m(\u001b[0m\u001b[33m &gt; 10 pages\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing 'text in the wild\\\\\\\".\\\\n\\\\nThe pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals , Scientific Articles , Laws &amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories \u001b[0m\u001b[33m(\u001b[0m\u001b[33m Financial Reports and Manuals \u001b[0m\u001b[33m)\u001b[0m\u001b[33m contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.\\\\n\\\\nWe did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mclose to 95%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m are published in English language. However, DocLayNet also contains a number of documents in other languages such as German \u001b[0m\u001b[33m(\u001b[0m\u001b[33m2.5%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, French \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1.0%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and Japanese \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1.0%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.\\\\n\\\\nTo ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.\\\\n\\\\n2 e.g. AAPL from https://www.annualreports.com/\\\\n\\\\nTable 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.\\\\n\\\\nIn order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. This entails the standard COCO ground-truth file \u001b[0m\u001b[33m(\u001b[0m\u001b[33min JSON format\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with the associated page images \u001b[0m\u001b[33m(\u001b[0m\u001b[33min PNG format, 1025 Ã— 1025 pixels\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates \u001b[0m\u001b[33m(\u001b[0m\u001b[33min JSON\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. All additional files are linked to the primary page images by their matching filenames.\\\\n\\\\nDespite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting \u001b[0m\u001b[33m(\u001b[0m\u001b[33msee Table 1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.\\\\n\\\\n## 4 ANNOTATION CAMPAIGN\\\\n\\\\nThe annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,\\\\n\\\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence \u001b[0m\u001b[33m(\u001b[0m\u001b[33mas % of row 'Total'\u001b[0m\u001b[33m)\u001b[0m\u001b[33m in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\\\n\\\\n|                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   | triple inter-annotator mAP @0.5-0.95 \u001b[0m\u001b[33m(\u001b[0m\u001b[33m%\u001b[0m\u001b[33m)\u001b[0m\u001b[33m   |\\\\n|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\\\\n| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\\\\n| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\\\\n| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\\\\n| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\\\\n| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\\\\n| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\\\\n| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\\\\n| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\\\\n| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\\\\n| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\\\\n| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\\\\n| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\\\\n| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\\\\n\\\\nFigure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells \u001b[0m\u001b[33m(\u001b[0m\u001b[33min darker shades\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.\\\\n\\\\n<!-- image -->\\\\n\\\\nwe distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.\\\\n\\\\nPhase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv 3 , government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.\\\\n\\\\nPreparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service \u001b[0m\u001b[33m(\u001b[0m\u001b[33mCCS\u001b[0m\u001b[33m)\u001b[0m\u001b[33m \u001b[0m\u001b[33m[\u001b[0m\u001b[33m22\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.\\\\n\\\\nPhase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula , List-item , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m the overall occurrence of the label, \u001b[0m\u001b[33m(\u001b[0m\u001b[33m2\u001b[0m\u001b[33m)\u001b[0m\u001b[33m the specificity of the label, \u001b[0m\u001b[33m(\u001b[0m\u001b[33m3\u001b[0m\u001b[33m)\u001b[0m\u001b[33m recognisability on a single page \u001b[0m\u001b[33m(\u001b[0m\u001b[33mi.e. no need for context from previous or next page\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and \u001b[0m\u001b[33m(\u001b[0m\u001b[33m4\u001b[0m\u001b[33m)\u001b[0m\u001b[33m overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on\\\\n\\\\n3 https://arxiv.org/\\\\n\\\\nthe textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.\\\\n\\\\nAt first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.\\\\n\\\\nObviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:\\\\n\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m2\u001b[0m\u001b[33m)\u001b[0m\u001b[33m A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m3\u001b[0m\u001b[33m)\u001b[0m\u001b[33m For every Caption , there must be exactly one corresponding Picture or Table .\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m4\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Connected sub-pictures are grouped together in one Picture object.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m5\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Formula numbers are included in a Formula object.\\\\n- \u001b[0m\u001b[33m(\u001b[0m\u001b[33m6\u001b[0m\u001b[33m)\u001b[0m\u001b[33m Emphasised text \u001b[0m\u001b[33m(\u001b[0m\u001b[33me.g. in italic or bold\u001b[0m\u001b[33m)\u001b[0m\u001b[33m at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.\\\\n\\\\nThe complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.\\\\n\\\\nPhase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets \u001b[0m\u001b[33m(\u001b[0m\u001b[33mblinded from the reference\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations\\\\n\\\\n<!-- image -->\\\\n\\\\n05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0\\\\n\\\\nFigure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.\\\\n\\\\nwere carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.\\\\n\\\\nPhase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement \u001b[0m\u001b[33m(\u001b[0m\u001b[33msee Table 1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. We wanted\\\\n\\\\nTable 2: Prediction performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of object detection networks on DocLayNet test set. The MRCNN \u001b[0m\u001b[33m(\u001b[0m\u001b[33mMask R-CNN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and FRCNN \u001b[0m\u001b[33m(\u001b[0m\u001b[33mFaster R-CNN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo \u001b[0m\u001b[33m(\u001b[0m\u001b[33mMask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, with default configurations. The YOLO implementation utilized was YOLOv5x6 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\\\n\\\\n|                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |\\\\n|----------------|---------|---------|---------|---------|--------|\\\\n|                |         | R50     | R101    | R101    | v5x6   |\\\\n| Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |\\\\n| Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |\\\\n| Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |\\\\n| List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |\\\\n| Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |\\\\n| Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |\\\\n| Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |\\\\n| Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |\\\\n| Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |\\\\n| Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |\\\\n| Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |\\\\n| All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |\\\\n\\\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\\\n\\\\n## 5 EXPERIMENTS\\\\n\\\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m and the availability of general frameworks such as detectron2 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m17\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\\\n\\\\nFigure 5: Prediction performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\\\n\\\\n<!-- image -->\\\\n\\\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\\\n\\\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m. These scores are computed by leveraging the evaluation code provided by the COCO API \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m.\\\\n\\\\n## Baselines for Object Detection\\\\n\\\\nIn Table 2, we present baseline experiments \u001b[0m\u001b[33m(\u001b[0m\u001b[33mgiven in mAP\u001b[0m\u001b[33m)\u001b[0m\u001b[33m on Mask R-CNN \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, Faster R-CNN \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, and YOLOv5 \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\\\n\\\\nTable 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.\\\\n\\\\n| Class-count    |   11 | 6       | 5       | 4       |\\\\n|----------------|------|---------|---------|---------|\\\\n| Caption        |   68 | Text    | Text    | Text    |\\\\n| Footnote       |   71 | Text    | Text    | Text    |\\\\n| Formula        |   60 | Text    | Text    | Text    |\\\\n| List-item      |   81 | Text    | 82      | Text    |\\\\n| Page-footer    |   62 | 62      | -       | -       |\\\\n| Page-header    |   72 | 68      | -       | -       |\\\\n| Picture        |   72 | 72      | 72      | 72      |\\\\n| Section-header |   68 | 67      | 69      | 68      |\\\\n| Table          |   82 | 83      | 82      | 82      |\\\\n| Text           |   85 | 84      | 84      | 84      |\\\\n| Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |\\\\n| Overall        |   72 | 73      | 78      | 77      |\\\\n\\\\n## Learning Curve\\\\n\\\\nOne of the fundamental questions related to any dataset is if it is 'large enough'. To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency \u001b[0m\u001b[33m(\u001b[0m\u001b[33mas discussed in Section 3\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, data augmentation methods \u001b[0m\u001b[33m[\u001b[0m\u001b[33m23\u001b[0m\u001b[33m]\u001b[0m\u001b[33m, or the addition of more document categories and styles.\\\\n\\\\n## Impact of Class Labels\\\\n\\\\nThe choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones \u001b[0m\u001b[33m(\u001b[0m\u001b[33me.g. Caption â†’ Text \u001b[0m\u001b[33m)\u001b[0m\u001b[33m or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of\\\\n\\\\nTable 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in /tildelow 10% point improvement.\\\\n\\\\n| Class-count Split   | 11   | 11   | 5   | 5    |\\\\n|---------------------|------|------|-----|------|\\\\n|                     | Doc  | Page | Doc | Page |\\\\n| Caption             | 68   | 83   |     |      |\\\\n| Footnote            | 71   | 84   |     |      |\\\\n| Formula             | 60   | 66   |     |      |\\\\n| List-item           | 81   | 88   | 82  | 88   |\\\\n| Page-footer         | 62   | 89   |     |      |\\\\n| Page-header         | 72   | 90   |     |      |\\\\n| Picture             | 72   | 82   | 72  | 82   |\\\\n| Section-header      | 68   | 83   | 69  | 83   |\\\\n| Table               | 82   | 89   | 82  | 90   |\\\\n| Text                | 85   | 91   | 84  | 90   |\\\\n| Title               | 77   | 81   |     |      |\\\\n| All                 | 72   | 84   | 78  | 87   |\\\\n\\\\nlists in PubLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mgrouped list-items\u001b[0m\u001b[33m)\u001b[0m\u001b[33m versus DocLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mseparate list-items\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.\\\\n\\\\n## Impact of Document Split in Train and Test Set\\\\n\\\\nMany documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains Ëœ 10% in mAP over the document-wise splitting. Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.\\\\n\\\\n## Dataset Comparison\\\\n\\\\nThroughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets \u001b[0m\u001b[33m(\u001b[0m\u001b[33mPubLayNet, DocBank and DocLayNet\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,\\\\n\\\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\\\n\\\\nTable 5: Prediction Performance \u001b[0m\u001b[33m(\u001b[0m\u001b[33mmAP@0.5-0.95\u001b[0m\u001b[33m)\u001b[0m\u001b[33m of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.\\\\n\\\\n|                 |            | Testing on   | Testing on   | Testing on   |\\\\n|-----------------|------------|--------------|--------------|--------------|\\\\n| Training on     | labels     | PLN          | DB           | DLN          |\\\\n| PubLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mPLN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | Figure     | 96           | 43           | 23           |\\\\n| PubLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mPLN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | Sec-header | 87           | -            | 32           |\\\\n|                 | Table      | 95           | 24           | 49           |\\\\n|                 | Text       | 96           | -            | 42           |\\\\n|                 | total      | 93           | 34           | 30           |\\\\n| DocBank \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    | Figure     | 77           | 71           | 31           |\\\\n| DocBank \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    | Table      | 19           | 65           | 22           |\\\\n| DocBank \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    | total      | 48           | 68           | 27           |\\\\n| DocLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDLN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | Figure     | 67           | 51           | 72           |\\\\n| DocLayNet \u001b[0m\u001b[33m(\u001b[0m\u001b[33mDLN\u001b[0m\u001b[33m)\u001b[0m\u001b[33m | Sec-header | 53           | -            | 68           |\\\\n|                 | Table      | 87           | 43           | 82           |\\\\n|                 | Text       | 77           | -            | 84           |\\\\n|                 | total      | 59           | 47           | 78           |\\\\n\\\\nSection-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists \u001b[0m\u001b[33m(\u001b[0m\u001b[33mby list-element vs. whole list objects\u001b[0m\u001b[33m)\u001b[0m\u001b[33m naturally decreases the mAP score for Text .\\\\n\\\\nFor comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.\\\\n\\\\n## Example Predictions\\\\n\\\\nTo conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.\\\\n\\\\n## 6 CONCLUSION\\\\n\\\\nIn this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.\\\\n\\\\nFrom the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation \u001b[0m\u001b[33m(\u001b[0m\u001b[33mthrough double and triple annotations\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.\\\\n\\\\nTo date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.\\\\n\\\\n## REFERENCES\\\\n\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m1\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Max GÃ¶bel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m2\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition \u001b[0m\u001b[33m(\u001b[0m\u001b[33mICDAR\u001b[0m\u001b[33m)\u001b[0m\u001b[33m , volume 01, pages 1404-1410, 2017.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m3\u001b[0m\u001b[33m]\u001b[0m\u001b[33m HervÃ© DÃ©jean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition \u001b[0m\u001b[33m(\u001b[0m\u001b[33mcTDaR\u001b[0m\u001b[33m)\u001b[0m\u001b[33m, April 2019. http://sac.founderit.com/.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m4\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m5\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition \u001b[0m\u001b[33m(\u001b[0m\u001b[33mIJDAR\u001b[0m\u001b[33m)\u001b[0m\u001b[33m , pages 1-11, 01 2022.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m6\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m7\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m8\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m9\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m10\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m11\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39\u001b[0m\u001b[33m(\u001b[0m\u001b[33m6\u001b[0m\u001b[33m)\u001b[0m\u001b[33m:1137-1149, 2017.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m12\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m13\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu\\\\n\\\\nFigure 6: Example layout predictions on selected pages from the DocLayNet test-set. \u001b[0m\u001b[33m(\u001b[0m\u001b[33mA, D\u001b[0m\u001b[33m)\u001b[0m\u001b[33m exhibit favourable results on coloured backgrounds. \u001b[0m\u001b[33m(\u001b[0m\u001b[33mB, C\u001b[0m\u001b[33m)\u001b[0m\u001b[33m show accurate list-item and paragraph differentiation despite densely-spaced lines. \u001b[0m\u001b[33m(\u001b[0m\u001b[33mE\u001b[0m\u001b[33m)\u001b[0m\u001b[33m demonstrates good table and figure distinction. \u001b[0m\u001b[33m(\u001b[0m\u001b[33mF\u001b[0m\u001b[33m)\u001b[0m\u001b[33m shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.\\\\n\\\\n<!-- image --\u001b[0m\u001b[33m>\u001b[0m\u001b[33m\\\\n\\\\nDiaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.\\\\n\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m14\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m15\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m16\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m17\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m18\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m19\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m20\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m21\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m22\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.\\\\n- \u001b[0m\u001b[33m[\u001b[0m\u001b[33m23\u001b[0m\u001b[33m]\u001b[0m\u001b[33m Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6\u001b[0m\u001b[33m(\u001b[0m\u001b[33m1\u001b[0m\u001b[33m)\u001b[0m\u001b[33m:60, 2019.\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"md_file\\\": \\\"/Users/dol/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/_cache/868f49ae1f0e66e82238a8aea43fd30b.md\\\",\\n  \\\"json_file\\\": \\\"/Users/dol/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/_cache/868f49ae1f0e66e82238a8aea43fd30b.json\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"md_file\\\": \\\"/Users/dol/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/_cache/868f49ae1f0e66e82238a8aea43fd30b.md\\\",\\n  \\\"json_file\\\": \\\"/Users/dol/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/_cache/868f49ae1f0e66e82238a8aea43fd30b.json\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"from_cache\\\": true,\\n  \\\"document_key\\\": \\\"868f49ae1f0e66e82238a8aea43fd30b\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"from_cache\\\": true,\\n  \\\"document_key\\\": \\\"868f49ae1f0e66e82238a8aea43fd30b\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:openai.agents:Error streaming response: 500: Internal server error: An unexpected error occurred.\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "500: Internal server error: An unexpected error occurred.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m vdb_id = vdb_id_resp.identifier\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Instruct the agent to ingest the data in the index and use it for querying.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_agent(\n\u001b[32m     12\u001b[39m     queries=[\n\u001b[32m     13\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIngest the document https://arxiv.org/pdf/2206.01062 into the vectordb \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvdb_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and answer the following questions: 1) how many pages were manually annotated?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     ],\n\u001b[32m     15\u001b[39m     extra_tools=[\n\u001b[32m     16\u001b[39m         FileSearchTool(vector_store_ids=[vdb_id])\n\u001b[32m     17\u001b[39m     ],\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mrun_agent\u001b[39m\u001b[34m(queries, extra_tools)\u001b[39m\n\u001b[32m     30\u001b[39m result = Runner.run_streamed(\n\u001b[32m     31\u001b[39m     agent,\n\u001b[32m     32\u001b[39m     prompt,\n\u001b[32m     33\u001b[39m     session=session,\n\u001b[32m     34\u001b[39m     run_config=RunConfig(model_provider=provider),\n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Print the events as they appear from the agent stream\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m result.stream_events():\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# We'll ignore the raw responses event deltas\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event.type == \u001b[33m\"\u001b[39m\u001b[33mraw_response_event\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/agents/result.py:233\u001b[39m, in \u001b[36mRunResultStreaming.stream_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m._cleanup_tasks()\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception:\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/agents/run.py:932\u001b[39m, in \u001b[36mAgentRunner._start_streaming\u001b[39m\u001b[34m(cls, starting_input, streamed_result, starting_agent, max_turns, hooks, context_wrapper, run_config, previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    921\u001b[39m     streamed_result._input_guardrails_task = asyncio.create_task(\n\u001b[32m    922\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails_with_queue(\n\u001b[32m    923\u001b[39m             starting_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m    929\u001b[39m         )\n\u001b[32m    930\u001b[39m     )\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn_streamed(\n\u001b[32m    933\u001b[39m         streamed_result,\n\u001b[32m    934\u001b[39m         current_agent,\n\u001b[32m    935\u001b[39m         hooks,\n\u001b[32m    936\u001b[39m         context_wrapper,\n\u001b[32m    937\u001b[39m         run_config,\n\u001b[32m    938\u001b[39m         should_run_agent_start_hooks,\n\u001b[32m    939\u001b[39m         tool_use_tracker,\n\u001b[32m    940\u001b[39m         all_tools,\n\u001b[32m    941\u001b[39m         previous_response_id,\n\u001b[32m    942\u001b[39m         conversation_id,\n\u001b[32m    943\u001b[39m     )\n\u001b[32m    944\u001b[39m     should_run_agent_start_hooks = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    946\u001b[39m     streamed_result.raw_responses = streamed_result.raw_responses + [\n\u001b[32m    947\u001b[39m         turn_result.model_response\n\u001b[32m    948\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/agents/run.py:1092\u001b[39m, in \u001b[36mAgentRunner._run_single_turn_streamed\u001b[39m\u001b[34m(cls, streamed_result, agent, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, all_tools, previous_response_id, conversation_id)\u001b[39m\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1081\u001b[39m     hooks.on_llm_start(context_wrapper, agent, filtered.instructions, filtered.input),\n\u001b[32m   1082\u001b[39m     (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m     ),\n\u001b[32m   1089\u001b[39m )\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# 1. Stream the output events\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1092\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m model.stream_response(\n\u001b[32m   1093\u001b[39m     filtered.instructions,\n\u001b[32m   1094\u001b[39m     filtered.input,\n\u001b[32m   1095\u001b[39m     model_settings,\n\u001b[32m   1096\u001b[39m     all_tools,\n\u001b[32m   1097\u001b[39m     output_schema,\n\u001b[32m   1098\u001b[39m     handoffs,\n\u001b[32m   1099\u001b[39m     get_model_tracing_impl(\n\u001b[32m   1100\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1101\u001b[39m     ),\n\u001b[32m   1102\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1103\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1104\u001b[39m     prompt=prompt_config,\n\u001b[32m   1105\u001b[39m ):\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, ResponseCompletedEvent):\n\u001b[32m   1107\u001b[39m         usage = (\n\u001b[32m   1108\u001b[39m             Usage(\n\u001b[32m   1109\u001b[39m                 requests=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1117\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Usage()\n\u001b[32m   1118\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/agents/models/openai_responses.py:185\u001b[39m, in \u001b[36mOpenAIResponsesModel.stream_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m    170\u001b[39m stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    171\u001b[39m     system_instructions,\n\u001b[32m    172\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m     prompt=prompt,\n\u001b[32m    181\u001b[39m )\n\u001b[32m    183\u001b[39m final_response: Response | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, ResponseCompletedEvent):\n\u001b[32m    187\u001b[39m         final_response = chunk.response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/openai/_streaming.py:147\u001b[39m, in \u001b[36mAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[_T]:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator:\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tx25/techxchange2025-lab3640-docling/.venv/lib/python3.12/site-packages/openai/_streaming.py:193\u001b[39m, in \u001b[36mAsyncStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    190\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    191\u001b[39m                 message = \u001b[33m\"\u001b[39m\u001b[33mAn error occurred during streaming\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[32m    194\u001b[39m                 message=message,\n\u001b[32m    195\u001b[39m                 request=\u001b[38;5;28mself\u001b[39m.response.request,\n\u001b[32m    196\u001b[39m                 body=data[\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    197\u001b[39m             )\n\u001b[32m    199\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m process_data(data=data, cast_to=cast_to, response=response)\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Ensure the entire stream is consumed\u001b[39;00m\n",
      "\u001b[31mAPIError\u001b[39m: 500: Internal server error: An unexpected error occurred."
     ]
    }
   ],
   "source": [
    "# Create a vectordb index in Llama Stack\n",
    "vdb_id_resp = lls_client.vector_dbs.register(\n",
    "    vector_db_id=(vdb_name := f\"testdb_{uuid.uuid4()}\"),\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=\"milvus\",\n",
    ")\n",
    "vdb_id = vdb_id_resp.identifier\n",
    "\n",
    "# Instruct the agent to ingest the data in the index and use it for querying.\n",
    "await run_agent(\n",
    "    queries=[\n",
    "        f\"Ingest the document https://arxiv.org/pdf/2206.01062 into the vectordb {vdb_id} and answer the following questions: 1) how many pages were manually annotated?\",\n",
    "    ],\n",
    "    extra_tools=[\n",
    "        FileSearchTool(vector_store_ids=[vdb_id])\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b20a21",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62856883",
   "metadata": {},
   "source": [
    "## 2. Create a document\n",
    "\n",
    "Another interesting use-case of the Docling MCP tools is the possibility to have a model writing complex document directly in the DoclingDocument format which can then be exported to a variety of outputs (e.g. Markdown, HTML, Latex, etc).\n",
    "\n",
    "In the following example we only provide the instructions on the topic and the key structure of the document and let the agent build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "798d81a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcp.client.streamable_http:Received session ID: 884d1f7194454cde91b2ddd5d6616b6c\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-06-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created session_id=6e5d1dab-0c11-46ce-88ed-6f7bb9bfcecb for Agent(Document Agent)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ðŸ‘¤ User&gt; You are an assistant that uses external tools.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Follow these rules strictly:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\">. At each reasoning step, you may call **at most one tool**.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">. If you need to use multiple tools, do it sequentially:  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Call exactly one tool.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Wait for its result.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">   - Then think again and, if needed, call another tool.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\">. Never call two or more tools in the same response.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\">. If no tool is needed, just provide your reasoning or final answer.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\">. Treat a missing tool result as blocking â€” do not continue reasoning until you receive the output.</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">If you ever attempt to call more than one tool at once, your answer will be rejected.  </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Always restrict yourself to **zero or one tool call per response**.</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">When the tools return a base64 image, simply create some markdown wrapper which allows to display it and take special care in not hallucinating any byte.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mðŸ‘¤ User> You are an assistant that uses external tools.  \u001b[0m\n",
       "\u001b[36mFollow these rules strictly:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[36m. At each reasoning step, you may call **at most one tool**.  \u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[36m. If you need to use multiple tools, do it sequentially:  \u001b[0m\n",
       "\u001b[36m   - Call exactly one tool.  \u001b[0m\n",
       "\u001b[36m   - Wait for its result.  \u001b[0m\n",
       "\u001b[36m   - Then think again and, if needed, call another tool.  \u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[36m. Never call two or more tools in the same response.  \u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[36m. If no tool is needed, just provide your reasoning or final answer.  \u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[36m. Treat a missing tool result as blocking â€” do not continue reasoning until you receive the output.\u001b[0m\n",
       "\n",
       "\u001b[36mIf you ever attempt to call more than one tool at once, your answer will be rejected.  \u001b[0m\n",
       "\u001b[36mAlways restrict yourself to **zero or one tool call per response**.\u001b[0m\n",
       "\n",
       "\u001b[36mWhen the tools return a base64 image, simply create some markdown wrapper which allows to display it and take special care in not hallucinating any byte.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Agent updated: Document Agent\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Agent updated: Document Agent\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸ¤– Assistant&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸ¤– Assistant>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Understood. I will adhere to the singleâ€‘toolâ€‘perâ€‘step rule and only invoke a tool when itâ€™s necessary for the task at hand. If no tool is required, Iâ€™ll provide the answer directly.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Understood. I will adhere to the singleâ€‘toolâ€‘perâ€‘step rule and only invoke a tool when itâ€™s necessary for the task at hand. If no tool is required, Iâ€™ll provide the answer directly.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ðŸ‘¤ User&gt; Create a new Docling document with the title </span><span style=\"color: #008080; text-decoration-color: #008080\">\"Open-Source Agentic AI\"</span><span style=\"color: #008080; text-decoration-color: #008080\">, a paragraph, and a list of the main applications.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mðŸ‘¤ User> Create a new Docling document with the title \u001b[0m\u001b[36m\"Open-Source Agentic AI\"\u001b[0m\u001b[36m, a paragraph, and a list of the main applications.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Agent updated: Document Agent\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Agent updated: Document Agent\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">create_new_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Prompt\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mcreate_new_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"Prompt\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"Error executing tool create_new_docling_document: 1 validation error for create_new_docling_documentArguments\\nprompt\\n  Field required \\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"Error executing tool create_new_docling_document: 1 validation error for create_new_docling_documentArguments\\nprompt\\n  Field required \u001b[0m\u001b[33m\\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">create_new_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Prompt\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"# Open-Source Agentic AI\\n\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\n\\n## Main Applications\\n\\n- Autonomous research assistants\\n- Personal productivity bots\\n- Adaptive tutoring systems\\n- Automated code generation\\n- Decision support agents\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mcreate_new_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"Prompt\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"# Open-Source Agentic AI\\n\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\n\\n## Main Applications\\n\\n- Autonomous research assistants\\n- Personal productivity bots\\n- Adaptive tutoring systems\\n- Automated code generation\\n- Decision support agents\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"Error executing tool create_new_docling_document: 1 validation error for create_new_docling_documentArguments\\nprompt\\n  Field required \\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"Error executing tool create_new_docling_document: 1 validation error for create_new_docling_documentArguments\\nprompt\\n  Field required \u001b[0m\u001b[33m\\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">create_new_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"prompt\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"# Open-Source Agentic AI\\n\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\n\\n## Main Applications\\n\\n- Autonomous research assistants\\n- Personal productivity bots\\n- Adaptive tutoring systems\\n- Automated code generation\\n- Decision support agents\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mcreate_new_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"prompt\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"# Open-Source Agentic AI\\n\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\n\\n## Main Applications\\n\\n- Autonomous research assistants\\n- Personal productivity bots\\n- Adaptive tutoring systems\\n- Automated code generation\\n- Decision support agents\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\",\\n  \\\"prompt\\\": \\\"# Open-Source Agentic AI\\\\n\\\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\\\n\\\\n## Main Applications\\\\n\\\\n- Autonomous research assistants\\\\n- Personal productivity bots\\\\n- Adaptive tutoring systems\\\\n- Automated code generation\\\\n- Decision support agents\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\",\\n  \\\"prompt\\\": \\\"# Open-Source Agentic AI\\\\n\\\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\\\n\\\\n## Main Applications\\\\n\\\\n- Autonomous research assistants\\\\n- Personal productivity bots\\\\n- Adaptive tutoring systems\\\\n- Automated code generation\\\\n- Decision support agents\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">add_title_to_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"title\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Open-Source Agentic AI\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33madd_title_to_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"title\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Open-Source Agentic AI\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">export_docling_document_to_markdown({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"max_size\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: null</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mexport_docling_document_to_markdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"max_size\"\u001b[0m\u001b[33m: null\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\",\\n  \\\"markdown\\\": \\\"\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\",\\n  \\\"markdown\\\": \\\"\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">add_paragraph_to_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"paragraph\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Open-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33madd_paragraph_to_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"paragraph\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Open-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">open_list_in_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mopen_list_in_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">add_list_items_to_list_in_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_items\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">[</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Autonomous research assistants\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Personal productivity bots\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Adaptive tutoring systems\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Automated code generation\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Decision support agents\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">]</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33madd_list_items_to_list_in_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"list_items\"\u001b[0m\u001b[33m: \u001b[0m\u001b[1;33m[\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Autonomous research assistants\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Personal productivity bots\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Adaptive tutoring systems\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Automated code generation\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Decision support agents\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[1;33m]\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">close_list_in_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mclose_list_in_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">export_docling_document_to_markdown({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"max_size\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: null</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mexport_docling_document_to_markdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"max_size\"\u001b[0m\u001b[33m: null\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"Error executing tool add_list_items_to_list_in_docling_document: No list is currently opened. Please open a list before adding list-items!\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"Error executing tool add_list_items_to_list_in_docling_document: No list is currently opened. Please open a list before adding list-items!\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\",\\n  \\\"markdown\\\": \\\"# Open-Source Agentic AI\\\\n\\\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\",\\n  \\\"markdown\\\": \\\"# Open-Source Agentic AI\\\\n\\\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">open_list_in_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mopen_list_in_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">add_list_items_to_list_in_docling_document({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_items\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">[</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Autonomous research assistants\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Personal productivity bots\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Adaptive tutoring systems\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Automated code generation\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_item_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Decision support agents\"</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">\"list_marker_text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"-\"</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">]</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33madd_list_items_to_list_in_docling_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"list_items\"\u001b[0m\u001b[33m: \u001b[0m\u001b[1;33m[\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Autonomous research assistants\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Personal productivity bots\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Adaptive tutoring systems\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Automated code generation\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m    \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"list_item_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"Decision support agents\"\u001b[0m\u001b[33m, \u001b[0m\u001b[33m\"list_marker_text\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"-\"\u001b[0m\u001b[1;33m}\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[1;33m]\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool was called: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">export_docling_document_to_markdown({</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"document_key\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span><span style=\"color: #808000; text-decoration-color: #808000\">\"f1a9f12563db40c8906ae369d3636214\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">  </span><span style=\"color: #808000; text-decoration-color: #808000\">\"max_size\"</span><span style=\"color: #808000; text-decoration-color: #808000\">: null</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool was called: \u001b[0m\u001b[1;33mexport_docling_document_to_markdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"document_key\"\u001b[0m\u001b[33m: \u001b[0m\u001b[33m\"f1a9f12563db40c8906ae369d3636214\"\u001b[0m\u001b[33m,\u001b[0m\n",
       "\u001b[33m  \u001b[0m\u001b[33m\"max_size\"\u001b[0m\u001b[33m: null\u001b[0m\n",
       "\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">-- Tool output: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">\"type\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span><span style=\"color: #808000; text-decoration-color: #808000\">\"{\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\",\\n  \\\"markdown\\\": \\\"# Open-Source Agentic AI\\\\n\\\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\\\n\\\\n- Autonomous research assistants\\\\n- Personal productivity bots\\\\n- Adaptive tutoring systems\\\\n- Automated code generation\\\\n- Decision support agents\\\"\\n}\"</span><span style=\"color: #808000; text-decoration-color: #808000\">,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"annotations\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null,</span><span style=\"color: #808000; text-decoration-color: #808000\">\"meta\"</span><span style=\"color: #808000; text-decoration-color: #808000\">:null</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m-- Tool output: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"type\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"text\"\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m\\n  \\\"document_key\\\": \\\"f1a9f12563db40c8906ae369d3636214\\\",\\n  \\\"markdown\\\": \\\"# Open-Source Agentic AI\\\\n\\\\nOpen-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems that can make decisions and act autonomously to achieve defined goals. These agents are built on open-source frameworks, enabling transparency, collaboration, and rapid innovation across diverse domains.\\\\n\\\\n- Autonomous research assistants\\\\n- Personal productivity bots\\\\n- Adaptive tutoring systems\\\\n- Automated code generation\\\\n- Decision support agents\\\"\\n\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\"annotations\"\u001b[0m\u001b[33m:null,\u001b[0m\u001b[33m\"meta\"\u001b[0m\u001b[33m:null\u001b[0m\u001b[1;33m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸ¤– Assistant&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸ¤– Assistant>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the Docling document you requested, rendered in markdown:\n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                    \n",
       " </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"># Open-Source Agentic AI</span><span style=\"background-color: #272822\">                                                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                    </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Open-Source Agentic AI refers to community-driven, self-governing artificial intelligence systems </span><span style=\"background-color: #272822\"> </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                    </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">-</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> Autonomous research assistants</span><span style=\"background-color: #272822\">                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">-</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> Personal productivity bots</span><span style=\"background-color: #272822\">                                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">-</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> Adaptive tutoring systems</span><span style=\"background-color: #272822\">                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">-</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> Automated code generation</span><span style=\"background-color: #272822\">                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">-</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> Decision support agents</span><span style=\"background-color: #272822\">                                                                          </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                    \n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "Here is the Docling document you requested, rendered in markdown:\n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                    \n",
       "\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m# Open-Source Agentic AI\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mOpen-Source\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAgentic\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAI\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrefers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcommunity-driven,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mself-governing\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34martificial\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mintelligence\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msystems\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34m-\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAutonomous\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresearch\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34massistants\u001b[0m\u001b[48;2;39;40;34m                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34m-\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mPersonal\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mproductivity\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbots\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34m-\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAdaptive\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtutoring\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msystems\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34m-\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAutomated\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcode\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mgeneration\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34m-\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mDecision\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msupport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34magents\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                    \n",
       "\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await run_agent(\n",
    "    queries=[\n",
    "        \"Create a new Docling document with the title \\\"Open-Source Agentic AI\\\", a paragraph, and a list of the main applications.\",\n",
    "    ],\n",
    "    extra_tools=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b707b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps: Where to Go from Here\n",
    "\n",
    "### Immediate actions\n",
    "\n",
    "1. **Experiment with your documents**\n",
    "   - Try documents with complex layouts\n",
    "   - Test with technical diagrams and charts\n",
    "   - Process multi-page reports with mixed content\n",
    "\n",
    "2. **Connect more agents**\n",
    "   - Try connecting more tools\n",
    "   - Search the documents to ingest via metadata\n",
    "   - Search the web for relevant documents\n",
    "   - Extract information from the documents\n",
    "\n",
    "3. **More ways to interact with tools**\n",
    "   - Use the Llama Stack playground UI for chatting with the agents\n",
    "   - Use other frameworks and ecosystems like Claude Desktop, BeeAI, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62057f76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec8b06",
   "metadata": {},
   "source": [
    "## Resources for Continued Learning\n",
    "\n",
    "### Official Documentation\n",
    "- **[Docling Documentation](https://github.com/docling-project/docling)**: Latest features and updates\n",
    "\n",
    "### Community Resources\n",
    "- Join the Docling community on GitHub\n",
    "- Share your implementations\n",
    "- Contribute improvements back to the project\n",
    "\n",
    "### Related Topics to Explore\n",
    "- Document Layout Analysis\n",
    "- Multimodal Embeddings\n",
    "- Visual Question Answering\n",
    "- Explainable AI Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c2c77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8a3b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techxchange2025-lab3640-docling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
